# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "unsloth",
#     "torch",
#     "transformers",
#     "datasets",
#     "trl",
#     "accelerate",
#     "peft",
#     "bitsandbytes",
#     "pandas",
#     "numpy",
#     "tensorboardx",
# ]
# ///

"""
ğŸš€ æ”¹è‰¯ç‰ˆ Gemma-3-1B GRPO æ¼¢å­—â†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›å­¦ç¿’
å¥èª­ç‚¹ãƒ»è¨˜å·ã‚’é©åˆ‡ã«å‡¦ç†ã™ã‚‹å ±é…¬é–¢æ•°ã‚’å®Ÿè£…
"""

import json
import torch
import os
import gc
import re
from datasets import Dataset, load_dataset, load_from_disk
from unsloth import FastModel
from unsloth.chat_templates import get_chat_template
from trl import GRPOTrainer, GRPOConfig

# L4 23GBæœ€å¤§æ´»ç”¨ã®ãŸã‚ã®ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:256,expandable_segments:True"
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# L4æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«è¨­å®š
max_seq_length = 320
lora_rank = 64

print("ğŸš€ æ”¹è‰¯ç‰ˆGRPO Gemma-3-1Bãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
model, tokenizer = FastModel.from_pretrained(
    model_name="unsloth/gemma-3-1b-pt-unsloth-bnb-4bit",
    max_seq_length=max_seq_length,
    load_in_4bit=True,
    dtype=torch.bfloat16,
)

tokenizer = get_chat_template(tokenizer, chat_template="gemma-3")

# é«˜æ€§èƒ½LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®š
print("âš™ï¸ é«˜æ€§èƒ½LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¨­å®šä¸­...")
model = FastModel.get_peft_model(
    model,
    finetune_vision_layers=False,
    finetune_language_layers=True,
    finetune_attention_modules=True,
    finetune_mlp_modules=True,
    r=lora_rank,
    lora_alpha=lora_rank * 2,
    lora_dropout=0.03,
    bias="none",
    random_state=3407,
)

def analyze_text_composition(text: str) -> dict:
    """ãƒ†ã‚­ã‚¹ãƒˆã®æ–‡å­—ç¨®æ§‹æˆã‚’è©³ç´°åˆ†æ"""
    katakana_chars = len([c for c in text if '\u30A0' <= c <= '\u30FF'])
    hiragana_chars = len([c for c in text if '\u3040' <= c <= '\u309F'])
    kanji_chars = len([c for c in text if '\u4E00' <= c <= '\u9FAF'])
    english_chars = len([c for c in text if c.isascii() and c.isalpha()])
    number_chars = len([c for c in text if c.isdigit()])
    
    # æ—¥æœ¬èªæ–‡å­—ã®ã¿ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå¥èª­ç‚¹ãƒ»è¨˜å·ã‚’é™¤å¤–ï¼‰
    japanese_chars = katakana_chars + hiragana_chars + kanji_chars + english_chars + number_chars
    
    return {
        'katakana': katakana_chars,
        'hiragana': hiragana_chars,
        'kanji': kanji_chars,
        'english': english_chars,
        'numbers': number_chars,
        'japanese_total': japanese_chars,
        'katakana_ratio': katakana_chars / max(japanese_chars, 1),
        'total_length': len(text)
    }

def improved_reward_function(prompt: str, response: str) -> float:
    """æ”¹è‰¯ç‰ˆå ±é…¬é–¢æ•°ï¼šå¥èª­ç‚¹ãƒ»è¨˜å·ã‚’é©åˆ‡ã«å‡¦ç†"""
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰å…ƒã®æ¼¢å­—æ–‡ã‚’æŠ½å‡º
    match = re.search(r'ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š(.+)', prompt)
    if not match:
        return -5.0
    
    original_text = match.group(1).strip()
    response = response.strip()
    
    if not response:
        return -5.0
    
    # ãƒ†ã‚­ã‚¹ãƒˆæ§‹æˆåˆ†æ
    original_analysis = analyze_text_composition(original_text)
    response_analysis = analyze_text_composition(response)
    
    total_reward = 0.0
    
    # 1. ã‚«ã‚¿ã‚«ãƒŠç´”åº¦å ±é…¬ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    katakana_ratio = response_analysis['katakana_ratio']
    if katakana_ratio >= 0.95:
        total_reward += 4.0  # 95%ä»¥ä¸Šã§é«˜å¾—ç‚¹
    elif katakana_ratio >= 0.85:
        total_reward += 2.0  # 85%ä»¥ä¸Šã§ä¸­å¾—ç‚¹
    elif katakana_ratio >= 0.70:
        total_reward += 0.5  # 70%ä»¥ä¸Šã§ä½å¾—ç‚¹
    else:
        total_reward -= 3.0  # 70%æœªæº€ã§ãƒšãƒŠãƒ«ãƒ†ã‚£
    
    # 2. ä¸é©åˆ‡æ–‡å­—ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    hiragana_penalty = response_analysis['hiragana'] * -0.8  # ã²ã‚‰ãŒãªãƒšãƒŠãƒ«ãƒ†ã‚£
    kanji_penalty = response_analysis['kanji'] * -1.2        # æ¼¢å­—ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆé‡ã„ï¼‰
    total_reward += hiragana_penalty + kanji_penalty
    
    # 3. é•·ã•é©åˆ‡æ€§å ±é…¬ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    length_ratio = response_analysis['total_length'] / max(original_analysis['total_length'], 1)
    if 0.8 <= length_ratio <= 2.5:
        total_reward += 2.5  # é©åˆ‡ãªé•·ã•
    elif 0.6 <= length_ratio <= 3.0:
        total_reward += 1.0  # ã‚„ã‚„é©åˆ‡
    elif length_ratio > 4.0:
        total_reward -= 4.0  # é•·ã™ãã‚‹
    elif length_ratio < 0.5:
        total_reward -= 3.0  # çŸ­ã™ãã‚‹
    
    # 4. å¥èª­ç‚¹ãƒ»è¨˜å·ä¿æŒå ±é…¬ï¼ˆæ–°è¦ï¼‰
    original_punctuation = len([c for c in original_text if c in 'ã€‚ã€ï¼ï¼Ÿãƒ»ã€Œã€ï¼ˆï¼‰'])
    response_punctuation = len([c for c in response if c in 'ã€‚ã€ï¼ï¼Ÿãƒ»ã€Œã€ï¼ˆï¼‰'])
    
    # å¥èª­ç‚¹ãŒé©åˆ‡ã«ä¿æŒã•ã‚Œã¦ã„ã‚‹å ´åˆã«ãƒœãƒ¼ãƒŠã‚¹
    if original_punctuation > 0:
        punctuation_ratio = response_punctuation / original_punctuation
        if 0.8 <= punctuation_ratio <= 1.2:
            total_reward += 1.5  # å¥èª­ç‚¹é©åˆ‡ä¿æŒãƒœãƒ¼ãƒŠã‚¹
        elif punctuation_ratio < 0.5:
            total_reward -= 1.0  # å¥èª­ç‚¹ä¸è¶³ãƒšãƒŠãƒ«ãƒ†ã‚£
    
    # 5. å®Œå…¨æ€§å ±é…¬ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    if response_analysis['japanese_total'] >= original_analysis['japanese_total'] * 0.8:
        total_reward += 1.5  # å†…å®¹ã®å®Œå…¨æ€§
    else:
        total_reward -= 2.0  # å†…å®¹ä¸è¶³
    
    # 6. ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆéµå®ˆå ±é…¬ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    # èª¬æ˜æ–‡ã‚„ä½™è¨ˆãªæ–‡è¨€ãŒãªã„ã‹ãƒã‚§ãƒƒã‚¯
    explanation_patterns = [
        r'å¤‰æ›ã™ã‚‹ã¨', r'ã‚«ã‚¿ã‚«ãƒŠã«ã™ã‚‹ã¨', r'èª­ã¿æ–¹ã¯', r'ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™',
        r'ç­”ãˆã¯', r'çµæœã¯', r'å¤‰æ›çµæœ', r'ã‚«ã‚¿ã‚«ãƒŠè¡¨è¨˜'
    ]
    
    has_explanation = any(re.search(pattern, response) for pattern in explanation_patterns)
    if has_explanation:
        total_reward -= 2.0  # èª¬æ˜æ–‡ãƒšãƒŠãƒ«ãƒ†ã‚£
    else:
        total_reward += 1.0  # ç›´æ¥å¤‰æ›ãƒœãƒ¼ãƒŠã‚¹
    
    # 7. æ–‡å­—ç¨®ä¸€è²«æ€§å ±é…¬ï¼ˆæ–°è¦ï¼‰
    # ã‚«ã‚¿ã‚«ãƒŠä»¥å¤–ã®æ–‡å­—ãŒæ··åœ¨ã—ã¦ã„ã‚‹å ´åˆã®ãƒšãƒŠãƒ«ãƒ†ã‚£èª¿æ•´
    if response_analysis['japanese_total'] > 0:
        consistency_score = response_analysis['katakana'] / response_analysis['japanese_total']
        if consistency_score >= 0.9:
            total_reward += 1.0  # é«˜ã„ä¸€è²«æ€§
        elif consistency_score < 0.7:
            total_reward -= 1.5  # ä½ã„ä¸€è²«æ€§
    
    # 8. ç‰¹æ®Šæ–‡å­—å‡¦ç†å ±é…¬ï¼ˆæ–°è¦ï¼‰
    # è‹±æ•°å­—ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if response_analysis['english'] > 0 or response_analysis['numbers'] > 0:
        # è‹±æ•°å­—ãŒã‚«ã‚¿ã‚«ãƒŠåŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ã€ãã®ã¾ã¾ä¿æŒã•ã‚Œã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡
        total_reward += 0.5  # è‹±æ•°å­—é©åˆ‡å‡¦ç†ãƒœãƒ¼ãƒŠã‚¹
    
    return max(total_reward, -10.0)  # æœ€ä½ã‚¹ã‚³ã‚¢åˆ¶é™

def format_gemma3_conversation(kanji_text: str, katakana_text: str) -> dict:
    """Gemma-3ç”¨ã®ä¼šè©±å½¢å¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    messages = [
        {
            "role": "user",
            "content": f"ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š{kanji_text}"
        },
        {
            "role": "assistant", 
            "content": katakana_text
        }
    ]
    return {"conversations": messages}

def load_grpo_dataset(file_paths: list[str], max_samples: int = 25000, sample_rate: float = 0.2) -> Dataset:
    """GRPOç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿"""
    data = []
    sample_count = 0
    total_lines = 0
    
    for file_path in file_paths:
        print(f"ğŸ“– GRPOç”¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f):
                    total_lines += 1
                    
                    if torch.rand(1).item() > sample_rate:
                        continue
                        
                    if sample_count >= max_samples:
                        print(f"âœ… æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•° {max_samples} ã«åˆ°é”")
                        break
                    
                    try:
                        item = json.loads(line.strip())
                        kanji_text = item["output"]
                        katakana_text = item["input"]
                        
                        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
                        if len(kanji_text) > 60 or len(katakana_text) > 60:
                            continue
                        
                        if len(kanji_text) < 5 or len(katakana_text) < 5:
                            continue
                        
                        # ã‚«ã‚¿ã‚«ãƒŠç´”åº¦äº‹å‰ãƒã‚§ãƒƒã‚¯ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
                        analysis = analyze_text_composition(katakana_text)
                        if analysis['katakana_ratio'] < 0.8:
                            continue
                        
                        conversation_data = format_gemma3_conversation(kanji_text, katakana_text)
                        
                        text = tokenizer.apply_chat_template(
                            conversation_data["conversations"], 
                            tokenize=False
                        )
                        
                        if len(tokenizer.encode(text)) <= max_seq_length:
                            data.append(conversation_data)
                            sample_count += 1
                            
                            if sample_count % 1000 == 0:
                                print(f"ğŸ“Š å‡¦ç†æ¸ˆã¿: {sample_count}/{total_lines} (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: {sample_rate*100:.1f}%)")
                        
                        if sample_count % 5000 == 0:
                            gc.collect()
                    
                    except (json.JSONDecodeError, KeyError) as e:
                        continue
                    except Exception as e:
                        if line_num % 20000 == 0:
                            print(f"âš ï¸ è¡Œ {line_num} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                        
                if sample_count >= max_samples:
                    break
                    
        except FileNotFoundError:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            continue
    
    print(f"âœ… æ”¹è‰¯ç‰ˆGRPOç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {len(data)} ã‚µãƒ³ãƒ—ãƒ«")
    return Dataset.from_list(data)

def apply_chat_template(examples):
    """ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨"""
    texts = []
    for conversation in examples["conversations"]:
        text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False)
        texts.append(text)
    return {"text": texts}

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
dataset_files = [
    "data/train_llm-jp-corpus-v3.jsonl",
    "data/train_wikipedia.jsonl"
]

print("ğŸ“š datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§GRPOç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ï¼†ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¸­...")
cache_dir = "cache/grpo_arrow"
if not os.path.exists(cache_dir):
    raw = load_dataset("json", data_files=dataset_files, split="train")
    raw = raw.filter(
        lambda ex: 5 <= len(ex["output"]) <= 60 and 5 <= len(ex["input"]) <= 60 and analyze_text_composition(ex["input"])["katakana_ratio"] >= 0.8,
        num_proc=4
    )
    raw = raw.map(
        lambda examples: {"conversations":[format_gemma3_conversation(o,i)["conversations"] for o,i in zip(examples["output"], examples["input"]) ]},
        batched=True, batch_size=1000, num_proc=4, remove_columns=["input","output","left_context"]
    )
    raw.save_to_disk(cache_dir)
dataset = load_from_disk(cache_dir)
print("ğŸ”„ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã‚’ãƒãƒƒãƒãƒ»ä¸¦åˆ—ã§å®Ÿè¡Œä¸­...")
dataset = dataset.map(
    lambda examples: {"text":[tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=False) for conv in examples["conversations"]]},
    batched=True, batch_size=1000, num_proc=4, remove_columns=["conversations"]
)

# ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª
print("\n=== ğŸ“‹ æ”¹è‰¯ç‰ˆGRPOã‚µãƒ³ãƒ—ãƒ«ç¢ºèª ===")
for i in range(min(2, len(dataset))):
    sample = dataset[i]
    print(f"ã‚µãƒ³ãƒ—ãƒ« {i+1}:")
    print(sample["text"][:200] + "..." if len(sample["text"]) > 200 else sample["text"])
    print("-" * 50)

# ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
gc.collect()
torch.cuda.empty_cache()

# æ”¹è‰¯ç‰ˆGRPO Trainerè¨­å®š
print("âš™ï¸ æ”¹è‰¯ç‰ˆGRPOTrainerã‚’è¨­å®šä¸­...")
trainer = GRPOTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    reward_function=improved_reward_function,
    args=GRPOConfig(
        dataset_text_field="text",
        per_device_train_batch_size=16,
        gradient_accumulation_steps=4,
        num_train_epochs=2,
        learning_rate=5e-5,
        logging_steps=5,
        save_steps=100,
        eval_steps=100,
        optim="adamw_torch_fused",
        weight_decay=0.01,
        lr_scheduler_type="cosine",
        warmup_steps=100,
        seed=3407,
        output_dir="./outputs/gemma3_1b_kanji2kana_grpo_improved",
        report_to="tensorboard",
        logging_dir="./logs/tensorboard_grpo_improved",
        dataloader_pin_memory=True,
        dataloader_num_workers=4,
        bf16=True,
        max_grad_norm=1.0,
        save_total_limit=3,
        # GRPOç‰¹æœ‰è¨­å®š
        grpo_alpha=1.0,
        grpo_beta=0.1,
        grpo_gamma=0.99,
    ),
)

print("ğŸš€ æ”¹è‰¯ç‰ˆGRPOè¨“ç·´é–‹å§‹...")
print("ğŸ”¥ æ”¹è‰¯ç‰ˆGRPOè¨­å®š:")
print(f"  - å ±é…¬é–¢æ•°: 8ã¤ã®æ”¹è‰¯ç‰ˆå ±é…¬è¦ç´ ")
print(f"  - å¥èª­ç‚¹ãƒ»è¨˜å·å‡¦ç†: é©åˆ‡ã«è€ƒæ…®")
print(f"  - ã‚«ã‚¿ã‚«ãƒŠç´”åº¦: æ—¥æœ¬èªæ–‡å­—ã®ã¿ã§è¨ˆç®—")
print(f"  - ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(dataset)}")
print(f"  - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: {sample_rate*100:.1f}%")

trainer_stats = trainer.train()

print(f"\nğŸ“Š æ”¹è‰¯ç‰ˆGRPOè¨“ç·´å®Œäº†!")
print(f"â±ï¸ è¨“ç·´æ™‚é–“: {trainer_stats.metrics['train_runtime']:.2f} ç§’")
print(f"ğŸ“ˆ æœ€çµ‚ãƒ­ã‚¹: {trainer_stats.metrics['train_loss']:.4f}")

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
save_dir = "model/gemma3_1b_kanji2kana_grpo_improved"
print(f"ğŸ’¾ æ”¹è‰¯ç‰ˆGRPOãƒ¢ãƒ‡ãƒ«ã‚’ {save_dir} ã«ä¿å­˜ä¸­...")
os.makedirs(save_dir, exist_ok=True)
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

print(f"âœ… æ”¹è‰¯ç‰ˆGRPOè¨“ç·´å®Œäº†ï¼")
print(f"ğŸ’¾ ä¿å­˜å…ˆ: {save_dir}")
print(f"ğŸ“Š ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: {len(dataset)} ã‚µãƒ³ãƒ—ãƒ«")

print("\n" + "="*70)
print("ğŸ¯ æ”¹è‰¯ç‰ˆGRPOå ±é…¬é–¢æ•°ã®ç‰¹å¾´:")
print("="*70)
print("1. ã‚«ã‚¿ã‚«ãƒŠç´”åº¦: å¥èª­ç‚¹ãƒ»è¨˜å·ã‚’é™¤å¤–ã—ã¦æ­£ç¢ºã«è¨ˆç®—")
print("2. å¥èª­ç‚¹ä¿æŒ: å…ƒæ–‡ã®å¥èª­ç‚¹ãŒé©åˆ‡ã«ä¿æŒã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯")
print("3. æ–‡å­—ç¨®ä¸€è²«æ€§: ã‚«ã‚¿ã‚«ãƒŠä»¥å¤–ã®æ–‡å­—æ··åœ¨ã‚’é©åˆ‡ã«ãƒšãƒŠãƒ«ãƒ†ã‚£")
print("4. ç‰¹æ®Šæ–‡å­—å‡¦ç†: è‹±æ•°å­—ã®é©åˆ‡ãªå‡¦ç†ã‚’è©•ä¾¡")
print("5. ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆéµå®ˆ: èª¬æ˜æ–‡ã‚’æ’é™¤ã—ç›´æ¥å¤‰æ›ã‚’ä¿ƒé€²")
print("6. é•·ã•é©åˆ‡æ€§: ã‚ˆã‚ŠæŸ”è»Ÿãªé•·ã•ç¯„å›²ã§è©•ä¾¡")
print("7. å®Œå…¨æ€§: å†…å®¹ã®æ¬ è½ã‚’é˜²æ­¢")
print("8. ä¸é©åˆ‡æ–‡å­—: ã²ã‚‰ãŒãªãƒ»æ¼¢å­—ã®æ®‹å­˜ã‚’å³æ ¼ã«ãƒšãƒŠãƒ«ãƒ†ã‚£")
print("")
print("âš¡ å¥èª­ç‚¹ãƒ»è¨˜å·å•é¡Œã‚’è§£æ±ºã—ãŸé«˜ç²¾åº¦GRPOå®Œæˆï¼") 