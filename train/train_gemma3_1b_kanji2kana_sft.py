# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "unsloth",
#     "torch",
#     "transformers",
#     "datasets",
#     "trl",
#     "accelerate",
#     "peft",
#     "bitsandbytes",
#     "pandas",
#     "numpy",
#     "tensorboardx",
# ]
# ///

"""
Gemma-3-1Bã§unslothã‚’ä½¿ã£ãŸæ¼¢å­—â†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå¤§ãã„ãŸã‚ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸãƒãƒƒãƒå‡¦ç†ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè£…
"""

import json
import torch
import os
from datasets import Dataset
from unsloth import FastModel
from unsloth.chat_templates import get_chat_template
from trl import SFTTrainer, SFTConfig

# L4 23GBæœ€å¤§æ´»ç”¨ã®ãŸã‚ã®ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:256,expandable_segments:True"
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# L4æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆ23GBæ´»ç”¨ï¼‰
max_seq_length = 384  # ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ã«æœ€é©ãªé•·ã•
lora_rank = 32        # LoRAãƒ©ãƒ³ã‚¯ã‚’å€å¢—ï¼ˆå“è³ªå‘ä¸Šï¼‰

print("ğŸš€ Gemma-3-1Bãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
model, tokenizer = FastModel.from_pretrained(
    model_name="unsloth/gemma-3-1b-pt-unsloth-bnb-4bit",
    max_seq_length=max_seq_length,
    load_in_4bit=True,
    dtype=torch.bfloat16,  # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªç²¾åº¦
)

# Gemma-3ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¨­å®š
tokenizer = get_chat_template(
    tokenizer,
    chat_template="gemma-3",
)

# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¿½åŠ 
print("âš™ï¸ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¨­å®šä¸­...")
model = FastModel.get_peft_model(
    model,
    finetune_vision_layers=False,
    finetune_language_layers=True,
    finetune_attention_modules=True,
    finetune_mlp_modules=True,
    r=lora_rank,
    lora_alpha=lora_rank * 2,
    lora_dropout=0.05,
    bias="none",
    random_state=3407,
)

def format_gemma3_conversation(kanji_text: str, katakana_text: str) -> dict:
    """Gemma-3ç”¨ã®ä¼šè©±å½¢å¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    messages = [
        {
            "role": "user",
            "content": f"ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š{kanji_text}"
        },
        {
            "role": "assistant", 
            "content": katakana_text
        }
    ]
    return {"conversations": messages}

def load_streaming_dataset(file_paths: list[str], max_samples: int = 10000, sample_rate: float = 0.1) -> Dataset:
    """
    å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã§åŠ¹ç‡çš„ã«èª­ã¿è¾¼ã‚€
    sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ï¼ˆ0.1 = 10%ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
    """
    data = []
    sample_count = 0
    total_lines = 0
    
    for file_path in file_paths:
        print(f"ğŸ“– èª­ã¿è¾¼ã¿é–‹å§‹: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f):
                    total_lines += 1
                    
                    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼šä¸€å®šã®ç¢ºç‡ã§ã‚¹ã‚­ãƒƒãƒ—
                    if torch.rand(1).item() > sample_rate:
                        continue
                        
                    if sample_count >= max_samples:
                        print(f"âœ… æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•° {max_samples} ã«åˆ°é”")
                        break
                    
                    try:
                        item = json.loads(line.strip())
                        kanji_text = item["output"]      # æ¼¢å­—æ··ã˜ã‚Šæ–‡ç« 
                        katakana_text = item["input"]    # ã‚«ã‚¿ã‚«ãƒŠæ–‡ç« 
                        
                        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
                        if len(kanji_text) > 100 or len(katakana_text) > 100:
                            continue
                        
                        if not kanji_text.strip() or not katakana_text.strip():
                            continue
                        
                        # Gemma-3ä¼šè©±å½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                        conversation_data = format_gemma3_conversation(kanji_text, katakana_text)
                        
                        # ãƒˆãƒ¼ã‚¯ãƒ³é•·ãƒã‚§ãƒƒã‚¯
                        text = tokenizer.apply_chat_template(
                            conversation_data["conversations"], 
                            tokenize=False
                        )
                        
                        if len(tokenizer.encode(text)) <= max_seq_length:
                            data.append(conversation_data)
                            sample_count += 1
                            
                            if sample_count % 500 == 0:
                                print(f"ğŸ“Š å‡¦ç†æ¸ˆã¿: {sample_count}/{total_lines} (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: {sample_rate*100:.1f}%)")
                    
                    except (json.JSONDecodeError, KeyError) as e:
                        continue
                    except Exception as e:
                        if line_num % 10000 == 0:
                            print(f"âš ï¸ è¡Œ {line_num} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                        
                if sample_count >= max_samples:
                    break
                    
        except FileNotFoundError:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            continue
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {len(data)} ã‚µãƒ³ãƒ—ãƒ« (ç·è¡Œæ•°: {total_lines})")
    return Dataset.from_list(data)

def apply_chat_template(examples):
    """ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨"""
    texts = []
    for conversation in examples["conversations"]:
        text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False)
        texts.append(text)
    return {"text": texts}

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
dataset_files = [
    "data/train_llm-jp-corpus-v3.jsonl",
    "data/train_wikipedia.jsonl"
]

print("ğŸ“š L4æœ€å¤§æ´»ç”¨ Gemma-3ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆä¸­...")
# L4ã®23GBãƒ¡ãƒ¢ãƒªã‚’æœ€å¤§é™æ´»ç”¨ã—ã¦ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
sample_rate = 0.25   # 25%ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆç´„8.5GBã®ãƒ‡ãƒ¼ã‚¿é‡ï¼‰
max_samples = 60000  # æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å¤§å¹…å¢—åŠ 

dataset = load_streaming_dataset(dataset_files, max_samples=max_samples, sample_rate=sample_rate)

# ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨
print("ğŸ”„ ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨ä¸­...")
dataset = dataset.map(apply_chat_template, batched=True, remove_columns=["conversations"])

# ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª
print("\n=== ğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª ===")
for i in range(min(2, len(dataset))):
    sample = dataset[i]
    print(f"ã‚µãƒ³ãƒ—ãƒ« {i+1}:")
    print(sample["text"][:300] + "..." if len(sample["text"]) > 300 else sample["text"])
    print("-" * 50)

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"ğŸ–¥ï¸ åˆæœŸGPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

# SFTTrainerè¨­å®šï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’é‡è¦–ï¼‰
print("âš™ï¸ SFTTrainerã‚’è¨­å®šä¸­...")
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    args=SFTConfig(
        dataset_text_field="text",
        per_device_train_batch_size=64,  # L4ã®23GBãƒ¡ãƒ¢ãƒªã‚’æœ€å¤§æ´»ç”¨ï¼ˆå€å¢—ï¼‰
        gradient_accumulation_steps=2,   # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º = 128
        warmup_steps=100,                # ã‚ˆã‚Šå¤šãã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        num_train_epochs=3,              # ã‚¨ãƒãƒƒã‚¯æ•°å¢—åŠ 
        learning_rate=8e-4,              # ã‚ˆã‚Šå¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã«å¯¾å¿œ
        logging_steps=10,               # ã‚ˆã‚Šç´°ã‹ã„ãƒ­ã‚°å‡ºåŠ›ï¼ˆTensorBoardç”¨ï¼‰
        save_steps=200,                 # ã‚ˆã‚Šé »ç¹ãªä¿å­˜
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="./outputs/gemma3_1b_kanji2kana_sft",
        report_to="tensorboard",        # TensorBoardæœ‰åŠ¹åŒ–
        logging_dir="./logs/tensorboard", # TensorBoardãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        dataloader_pin_memory=True,   # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æœ€é©åŒ–
        dataloader_num_workers=8,     # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸¦åˆ—åº¦ã‚’å€å¢—
        gradient_checkpointing=False,  # GPUé«˜é€ŸåŒ–
        bf16=True,                    # Gemma-3ç”¨ã«bfloat16ã‚’ä½¿ç”¨
    ),
)

print("ğŸš€ Gemma-3-1B SFTè¨“ç·´é–‹å§‹...")
trainer_stats = trainer.train()

# è¨“ç·´çµ±è¨ˆè¡¨ç¤º
print(f"\nğŸ“Š è¨“ç·´å®Œäº†!")
print(f"â±ï¸ è¨“ç·´æ™‚é–“: {trainer_stats.metrics['train_runtime']:.2f} ç§’")
print(f"ğŸ“ˆ æœ€çµ‚ãƒ­ã‚¹: {trainer_stats.metrics['train_loss']:.4f}")

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆmodel/é…ä¸‹ã«ä¿å­˜ï¼‰
save_dir = "model/gemma3_1b_kanji2kana_sft"
print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ {save_dir} ã«ä¿å­˜ä¸­...")
os.makedirs(save_dir, exist_ok=True)
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

# float16å½¢å¼ã§ã‚‚ä¿å­˜
print("ğŸ’¾ float16å½¢å¼ã§ã‚‚ä¿å­˜ä¸­...")
model.save_pretrained_merged(f"{save_dir}_merged", tokenizer)

# ãƒ†ã‚¹ãƒˆæ¨è«–
print("\n=== ğŸ§ª ãƒ†ã‚¹ãƒˆæ¨è«– ===")
# æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ
FastModel.for_inference(model)

def test_gemma3_inference(text: str) -> str:
    """Gemma-3ã§ã®ãƒ†ã‚¹ãƒˆæ¨è«–"""
    messages = [{
        "role": "user",
        "content": f"ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š{text}"
    }]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            temperature=0.3,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    # å…¥åŠ›éƒ¨åˆ†ã‚’é™¤å»ã—ã¦å‡ºåŠ›ã®ã¿å–å¾—
    generated_tokens = outputs[0][inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    return result.strip()

# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
test_cases = [
    "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã€‚",
    "ç§ã¯å­¦ç”Ÿã§ã™ã€‚", 
    "æ±äº¬é§…ã«è¡Œãã¾ã™ã€‚",
    "ç¾ã—ã„èŠ±ãŒå’²ã„ã¦ã„ã¾ã™ã€‚",
    "æ˜æ—¥ã¯é›¨ãŒé™ã‚‹ã§ã—ã‚‡ã†ã€‚"
]

print("ğŸ” Gemma-3æ¨è«–ãƒ†ã‚¹ãƒˆçµæœ:")
for i, test_case in enumerate(test_cases, 1):
    try:
        result = test_gemma3_inference(test_case)
        print(f"{i}. å…¥åŠ›: {test_case}")
        print(f"   å‡ºåŠ›: {result}")
        print("-" * 40)
    except Exception as e:
        print(f"{i}. å…¥åŠ›: {test_case}")
        print(f"   ã‚¨ãƒ©ãƒ¼: {e}")
        print("-" * 40)

# æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
if torch.cuda.is_available():
    print(f"\nğŸ–¥ï¸ æœ€çµ‚GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    print(f"ğŸ“‹ GPUæœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")

print(f"\nâœ… Gemma-3-1B Kanji2Kana SFTè¨“ç·´å®Œäº†ï¼")
print(f"ğŸ’¾ ä¿å­˜å…ˆ: {save_dir}")
print(f"ğŸ“Š ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: {len(dataset)} ã‚µãƒ³ãƒ—ãƒ« (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: {sample_rate*100:.1f}%)")

# RAMä½¿ç”¨é‡ã«é–¢ã™ã‚‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹
print("\n" + "="*60)
print("ğŸ’¡ L4æœ€é©åŒ–æ¸ˆã¿è¨­å®šã®è©³ç´°:")
print("="*60)
print("ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡:", f"{sample_rate*100:.1f}% (å‰å›ã®3å€)")
print("å‡¦ç†ãƒ‡ãƒ¼ã‚¿é‡:", f"ç´„{34 * sample_rate:.1f}GB")
print("ãƒãƒƒãƒã‚µã‚¤ã‚º:", "8 (å‰å›ã®8å€)")
print("å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º:", "16 (å‰å›ã®2å€)")
print("ã‚¨ãƒãƒƒã‚¯æ•°:", "2 (å‰å›ã®2/3)")
print("")
print("ğŸš€ äºˆæƒ³ã•ã‚Œã‚‹æ”¹å–„:")
print("- è¨“ç·´é€Ÿåº¦: ç´„8-10å€é«˜é€ŸåŒ–")
print("- GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: 4% â†’ 40-60%")
print("- ç·è¨“ç·´æ™‚é–“: 9æ™‚é–“ â†’ 1-2æ™‚é–“")
print("")
print("âš¡ L4ã®æ€§èƒ½ã‚’æœ€å¤§é™æ´»ç”¨ã™ã‚‹è¨­å®šã«æœ€é©åŒ–å®Œäº†ï¼") 