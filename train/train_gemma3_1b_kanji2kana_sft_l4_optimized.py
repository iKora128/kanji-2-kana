# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "unsloth",
#     "torch",
#     "transformers",
#     "datasets",
#     "trl",
#     "accelerate",
#     "peft",
#     "bitsandbytes",
#     "pandas",
#     "numpy",
#     "tensorboardx",
# ]
# ///

"""
ğŸš€ Gemma-3-1B SFT L4æœ€é©åŒ–ç‰ˆ - 23GBãƒ¡ãƒ¢ãƒªã‚’æœ€å¤§é™æ´»ç”¨
æ¼¢å­—â†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ã‚¿ã‚¹ã‚¯ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§å¹…æ”¹å–„ã—ãŸå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import torch
import os
import gc
from datasets import Dataset, load_dataset, load_from_disk
from unsloth import FastModel
from unsloth.chat_templates import get_chat_template
from trl import SFTTrainer, SFTConfig

# L4 23GBæœ€å¤§æ´»ç”¨ã®ãŸã‚ã®ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:256,expandable_segments:True"
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# L4æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆ23GBæ´»ç”¨ï¼‰
max_seq_length = 320   # ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ã«æœ€é©ãªé•·ã•ï¼ˆåŠ¹ç‡é‡è¦–ï¼‰
lora_rank = 64         # LoRAãƒ©ãƒ³ã‚¯ã‚’å¤§å¹…å¢—åŠ ï¼ˆå“è³ªå‘ä¸Šï¼‰

print("ğŸš€ L4æœ€é©åŒ– Gemma-3-1Bãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
model, tokenizer = FastModel.from_pretrained(
    model_name="unsloth/gemma-3-1b-pt-unsloth-bnb-4bit",
    max_seq_length=max_seq_length,
    load_in_4bit=True,
    dtype=torch.bfloat16,  # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªç²¾åº¦
)

# Gemma-3ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¨­å®š
tokenizer = get_chat_template(
    tokenizer,
    chat_template="gemma-3",
)

# é«˜æ€§èƒ½LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®š
print("âš™ï¸ é«˜æ€§èƒ½LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¨­å®šä¸­...")
model = FastModel.get_peft_model(
    model,
    finetune_vision_layers=False,
    finetune_language_layers=True,
    finetune_attention_modules=True,
    finetune_mlp_modules=True,
    r=lora_rank,
    lora_alpha=lora_rank * 2,      # ã‚ˆã‚Šç©æ¥µçš„ãªå­¦ç¿’
    lora_dropout=0.03,             # éå­¦ç¿’é˜²æ­¢ã‚’å°‘ã—ç·©å’Œ
    bias="none",
    random_state=3407,
)

def format_gemma3_conversation(kanji_text: str, katakana_text: str) -> dict:
    """Gemma-3ç”¨ã®ä¼šè©±å½¢å¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    messages = [
        {
            "role": "user",
            "content": f"ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š{kanji_text}"
        },
        {
            "role": "assistant", 
            "content": katakana_text
        }
    ]
    return {"conversations": messages}

def load_optimized_streaming_dataset(file_paths: list[str], max_samples: int = 80000, sample_rate: float = 0.4) -> Dataset:
    """
    L4æœ€é©åŒ–ç‰ˆï¼šå¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åŠ¹ç‡çš„ã«èª­ã¿è¾¼ã‚€
    sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ï¼ˆ0.4 = 40%ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
    """
    data = []
    sample_count = 0
    total_lines = 0
    
    for file_path in file_paths:
        print(f"ğŸ“– å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f):
                    total_lines += 1
                    
                    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼šã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                    if torch.rand(1).item() > sample_rate:
                        continue
                        
                    if sample_count >= max_samples:
                        print(f"âœ… æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•° {max_samples} ã«åˆ°é”")
                        break
                    
                    try:
                        item = json.loads(line.strip())
                        kanji_text = item["output"]      # æ¼¢å­—æ··ã˜ã‚Šæ–‡ç« 
                        katakana_text = item["input"]    # ã‚«ã‚¿ã‚«ãƒŠæ–‡ç« 
                        
                        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆå°‘ã—ç·©å’Œã—ã¦å¤šæ§˜æ€§ç¢ºä¿ï¼‰
                        if len(kanji_text) > 80 or len(katakana_text) > 80:
                            continue
                        
                        if len(kanji_text) < 3 or len(katakana_text) < 3:
                            continue
                        
                        if not kanji_text.strip() or not katakana_text.strip():
                            continue
                        
                        # ã‚«ã‚¿ã‚«ãƒŠç´”åº¦äº‹å‰ãƒã‚§ãƒƒã‚¯
                        katakana_chars = len([c for c in katakana_text if '\u30A0' <= c <= '\u30FF'])
                        total_chars = len([c for c in katakana_text if not c.isspace()])
                        if total_chars > 0 and katakana_chars / total_chars < 0.7:
                            continue
                        
                        # Gemma-3ä¼šè©±å½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                        conversation_data = format_gemma3_conversation(kanji_text, katakana_text)
                        
                        # ãƒˆãƒ¼ã‚¯ãƒ³é•·ãƒã‚§ãƒƒã‚¯
                        text = tokenizer.apply_chat_template(
                            conversation_data["conversations"], 
                            tokenize=False
                        )
                        
                        if len(tokenizer.encode(text)) <= max_seq_length:
                            data.append(conversation_data)
                            sample_count += 1
                            
                            if sample_count % 1000 == 0:
                                print(f"ğŸ“Š å‡¦ç†æ¸ˆã¿: {sample_count}/{total_lines} (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: {sample_rate*100:.1f}%)")
                        
                        # ãƒ¡ãƒ¢ãƒªç®¡ç†
                        if sample_count % 10000 == 0:
                            gc.collect()
                    
                    except (json.JSONDecodeError, KeyError) as e:
                        continue
                    except Exception as e:
                        if line_num % 20000 == 0:
                            print(f"âš ï¸ è¡Œ {line_num} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                        
                if sample_count >= max_samples:
                    break
                    
        except FileNotFoundError:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            continue
    
    print(f"âœ… L4æœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {len(data)} ã‚µãƒ³ãƒ—ãƒ« (ç·è¡Œæ•°: {total_lines})")
    return Dataset.from_list(data)

def apply_chat_template(examples):
    """ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨"""
    texts = []
    for conversation in examples["conversations"]:
        text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False)
        texts.append(text)
    return {"text": texts}

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
dataset_files = [
    "data/train_llm-jp-corpus-v3.jsonl",
    "data/train_wikipedia.jsonl"
]

print("ğŸ“š datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ï¼†ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¸­...")
cache_dir = "cache/sft_l4_arrow"
if not os.path.exists(cache_dir):
    raw = load_dataset("json", data_files=dataset_files, split="train")
    raw = raw.filter(
        lambda ex: 3 <= len(ex["output"]) <= 80 and 3 <= len(ex["input"]) <= 80 and sum(1 for c in ex["input"] if '\u30A0' <= c <= '\u30FF') / max(len(ex["input"].replace(" ","")),1) >= 0.7,
        num_proc=16
    )
    raw = raw.map(
        lambda examples: {"conversations":[format_gemma3_conversation(o,i)["conversations"] for o,i in zip(examples["output"], examples["input"])]},
        batched=True, batch_size=1000, num_proc=16, remove_columns=["input","output","left_context"]
    )
    raw.save_to_disk(cache_dir)
dataset = load_from_disk(cache_dir)
print("ğŸ”„ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã‚’ãƒãƒƒãƒãƒ»ä¸¦åˆ—ã§å®Ÿè¡Œä¸­...")
dataset = dataset.map(
    lambda examples: {"text":[tokenizer.apply_chat_template(conv, tokenize=False) for conv in examples["conversations"]]},
    batched=True, batch_size=1000, num_proc=16, remove_columns=["conversations"]
)

# ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª
print("\n=== ğŸ“‹ L4æœ€é©åŒ–ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª ===")
for i in range(min(3, len(dataset))):
    sample = dataset[i]
    print(f"ã‚µãƒ³ãƒ—ãƒ« {i+1}:")
    print(sample["text"][:250] + "..." if len(sample["text"]) > 250 else sample["text"])
    print("-" * 50)

# ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
gc.collect()
torch.cuda.empty_cache()

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
if torch.cuda.is_available():
    print(f"ğŸ–¥ï¸ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¾ŒGPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

# L4æœ€é©åŒ–SFTTrainerè¨­å®šï¼ˆ23GBæ´»ç”¨ï¼‰
print("âš™ï¸ L4æœ€å¤§æ´»ç”¨SFTTrainerã‚’è¨­å®šä¸­...")
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    args=SFTConfig(
        dataset_text_field="text",
        per_device_train_batch_size=96,  # L4ã®23GBãƒ¡ãƒ¢ãƒªã‚’æœ€å¤§æ´»ç”¨ï¼ˆå¤§å¹…å¢—åŠ ï¼‰
        gradient_accumulation_steps=2,   # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º = 192
        warmup_steps=150,                # ã‚ˆã‚Šå¤šãã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        num_train_epochs=30,              # ã‚¨ãƒãƒƒã‚¯æ•°å¢—åŠ 
        learning_rate=1e-3,              # ã‚ˆã‚Šå¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã«å¯¾å¿œ
        logging_steps=5,                 # ã‚ˆã‚Šç´°ã‹ã„ãƒ­ã‚°å‡ºåŠ›ï¼ˆTensorBoardç”¨ï¼‰
        save_steps=100,                  # ã‚ˆã‚Šé »ç¹ãªä¿å­˜
        eval_steps=100,                  # è©•ä¾¡ã‚¹ãƒ†ãƒƒãƒ—è¿½åŠ 
        optim="adamw_torch_fused",       # ã‚ˆã‚Šé«˜é€Ÿãªã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        weight_decay=0.02,               # éå­¦ç¿’é˜²æ­¢å¼·åŒ–
        lr_scheduler_type="cosine",      # ã‚³ã‚µã‚¤ãƒ³ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        seed=3407,
        output_dir="./outputs/gemma3_1b_kanji2kana_sft_l4_optimized",
        report_to="tensorboard",         # TensorBoardæœ‰åŠ¹åŒ–
        logging_dir="./logs/tensorboard_sft_l4", # TensorBoardãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        dataloader_pin_memory=True,      # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æœ€é©åŒ–
        dataloader_num_workers=8,        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸¦åˆ—åº¦
        gradient_checkpointing=False,    # GPUé«˜é€ŸåŒ–ï¼ˆãƒ¡ãƒ¢ãƒªååˆ†ãªãŸã‚ï¼‰
        bf16=True,                       # Gemma-3ç”¨ã«bfloat16ã‚’ä½¿ç”¨
        max_grad_norm=1.0,               # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        remove_unused_columns=False,     # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿æŒ
        save_total_limit=5,              # ä¿å­˜ãƒ¢ãƒ‡ãƒ«æ•°åˆ¶é™
    ),
)

print("ğŸš€ L4æœ€å¤§æ´»ç”¨ Gemma-3-1B SFTè¨“ç·´é–‹å§‹...")
print("ğŸ”¥ L4æœ€é©åŒ–è¨­å®š:")
print(f"  - ãƒãƒƒãƒã‚µã‚¤ã‚º: 96 (å…ƒã®3å€)")
print(f"  - å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: 192 (å…ƒã®6å€)")
print(f"  - ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(dataset)} (å…ƒã®ç´„2.7å€)")
print(f"  - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: {sample_rate*100:.1f}% (å…ƒã®ç´„2.7å€)")
print(f"  - LoRAãƒ©ãƒ³ã‚¯: {lora_rank} (å…ƒã®4å€)")
print(f"  - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {max_seq_length} (åŠ¹ç‡åŒ–)")

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
if torch.cuda.is_available():
    print(f"ğŸ–¥ï¸ è¨“ç·´å‰GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

trainer_stats = trainer.train()

# è¨“ç·´çµ±è¨ˆè¡¨ç¤º
print(f"\nğŸ“Š L4æœ€é©åŒ–SFTè¨“ç·´å®Œäº†!")
print(f"â±ï¸ è¨“ç·´æ™‚é–“: {trainer_stats.metrics['train_runtime']:.2f} ç§’")
print(f"ğŸ“ˆ æœ€çµ‚ãƒ­ã‚¹: {trainer_stats.metrics['train_loss']:.4f}")

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆmodel/é…ä¸‹ã«ä¿å­˜ï¼‰
save_dir = "model/gemma3_1b_kanji2kana_sft_l4_optimized"
print(f"ğŸ’¾ æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ {save_dir} ã«ä¿å­˜ä¸­...")
os.makedirs(save_dir, exist_ok=True)
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

# float16å½¢å¼ã§ã‚‚ä¿å­˜
print("ğŸ’¾ float16å½¢å¼ã§ã‚‚ä¿å­˜ä¸­...")
model.save_pretrained_merged(f"{save_dir}_merged", tokenizer)

# ãƒ†ã‚¹ãƒˆæ¨è«–
print("\n=== ğŸ§ª L4æœ€é©åŒ–SFTè¨“ç·´å¾Œãƒ†ã‚¹ãƒˆæ¨è«– ===")
# æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ
FastModel.for_inference(model)

def test_l4_optimized_inference(text: str) -> str:
    """L4æœ€é©åŒ–SFTè¨“ç·´å¾Œã®ãƒ†ã‚¹ãƒˆæ¨è«–"""
    messages = [{
        "role": "user",
        "content": f"ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š{text}"
    }]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.2,                # ã‚ˆã‚Šæ±ºå®šçš„ãªç”Ÿæˆ
            top_p=0.85,                     # ã‚ˆã‚Šé›†ä¸­ã—ãŸå€™è£œé¸æŠ
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            repetition_penalty=1.05,        # ç¹°ã‚Šè¿”ã—é˜²æ­¢
        )
    
    # å…¥åŠ›éƒ¨åˆ†ã‚’é™¤å»ã—ã¦å‡ºåŠ›ã®ã¿å–å¾—
    generated_tokens = outputs[0][inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    return result.strip()

# æ‹¡å¼µãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
test_cases = [
    "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã€‚",
    "ç§ã¯å­¦ç”Ÿã§ã™ã€‚", 
    "æ±äº¬é§…ã«è¡Œãã¾ã™ã€‚",
    "ç¾ã—ã„èŠ±ãŒå’²ã„ã¦ã„ã¾ã™ã€‚",
    "æ˜æ—¥ã¯é›¨ãŒé™ã‚‹ã§ã—ã‚‡ã†ã€‚",
    "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ã„ã¾ã™ã€‚",
    "æ–°å¹¹ç·šã§å¤§é˜ªã¸è¡Œãã€‚",
    "ç§‘å­¦æŠ€è¡“ã®ç™ºå±•ãŒè‘—ã—ã„ã€‚",
    "å›½éš›ä¼šè­°ãŒé–‹å‚¬ã•ã‚Œã‚‹ã€‚",
    "äººå·¥çŸ¥èƒ½ãŒæ™®åŠã—ã¦ã„ã‚‹ã€‚"
]

print("ğŸ” L4æœ€é©åŒ–SFTè¨“ç·´å¾Œã®æ¨è«–ãƒ†ã‚¹ãƒˆçµæœ:")
for i, test_case in enumerate(test_cases, 1):
    try:
        result = test_l4_optimized_inference(test_case)
        print(f"{i}. å…¥åŠ›: {test_case}")
        print(f"   å‡ºåŠ›: {result}")
        
        # è©³ç´°è©•ä¾¡
        katakana_chars = len([c for c in result if '\u30A0' <= c <= '\u30FF'])
        total_chars = len([c for c in result if not c.isspace()])
        katakana_ratio = katakana_chars / max(total_chars, 1)
        length_ratio = len(result) / len(test_case) if test_case else 0
        
        print(f"   ã‚«ã‚¿ã‚«ãƒŠç‡: {katakana_ratio:.1%}, é•·ã•æ¯”ç‡: {length_ratio:.2f}")
        print("-" * 50)
    except Exception as e:
        print(f"{i}. å…¥åŠ›: {test_case}")
        print(f"   ã‚¨ãƒ©ãƒ¼: {e}")
        print("-" * 50)

# æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
if torch.cuda.is_available():
    print(f"\nğŸ–¥ï¸ æœ€çµ‚GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    print(f"ğŸ“‹ GPUæœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
    print(f"ğŸ“ˆ ãƒ¡ãƒ¢ãƒªæ´»ç”¨ç‡: {torch.cuda.max_memory_allocated() / (23 * 1024**3) * 100:.1f}%")

print(f"\nâœ… L4æœ€é©åŒ– Gemma-3-1B Kanji2Kana SFTè¨“ç·´å®Œäº†ï¼")
print(f"ğŸ’¾ ä¿å­˜å…ˆ: {save_dir}")
print(f"ğŸ“Š ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: {len(dataset)} ã‚µãƒ³ãƒ—ãƒ« (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: {sample_rate*100:.1f}%)")

print("\n" + "="*70)
print("ğŸš€ L4 SFTæœ€é©åŒ–ã®è©³ç´°æ”¹å–„ç‚¹:")
print("="*70)
print("ã€ãƒ¡ãƒ¢ãƒªæ´»ç”¨ã€‘")
print("- ãƒãƒƒãƒã‚µã‚¤ã‚º: 32 â†’ 96 (3å€å¢—)")
print("- å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: 32 â†’ 192 (6å€å¢—)")
print("- ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: 384 â†’ 320 (åŠ¹ç‡åŒ–)")
print("- LoRAãƒ©ãƒ³ã‚¯: 32 â†’ 64 (2å€å¢—)")
print("")
print("ã€ãƒ‡ãƒ¼ã‚¿å“è³ªã€‘")
print(f"- ã‚µãƒ³ãƒ—ãƒ«æ•°: 30,000 â†’ {len(dataset)} (ç´„2.7å€å¢—)")
print("- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: 25% â†’ 40% (1.6å€å¢—)")
print("- å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¼·åŒ–")
print("")
print("ã€å­¦ç¿’åŠ¹ç‡ã€‘")
print("- ã‚¨ãƒãƒƒã‚¯æ•°: 2 â†’ 3 (1.5å€å¢—)")
print("- ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼: adamw_8bit â†’ adamw_torch_fused")
print("- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼: linear â†’ cosine")
print("- ã‚ˆã‚Šç´°ã‹ã„è©•ä¾¡ãƒ»ä¿å­˜")
print("")
print("ğŸ¯ æœŸå¾…ã•ã‚Œã‚‹å¤§å¹…æ”¹å–„:")
print("- å­¦ç¿’é€Ÿåº¦: ç´„6å€é«˜é€ŸåŒ–")
print("- GPUãƒ¡ãƒ¢ãƒªæ´»ç”¨ç‡: 15% â†’ 70-85%")
print("- ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ç²¾åº¦ã®å‘ä¸Š")
print("- ã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’")
print("")
print("âš¡ L4ã®23GBãƒ¡ãƒ¢ãƒªã‚’æœ€å¤§é™æ´»ç”¨ã™ã‚‹è¨­å®šå®Œäº†ï¼")

# RAMä½¿ç”¨é‡ã«é–¢ã™ã‚‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹
print("\n" + "="*60)
print("ğŸ’¡ RAMä½¿ç”¨é‡ã«ã¤ã„ã¦:")
print("="*60)
print(f"å‡¦ç†ãƒ‡ãƒ¼ã‚¿é‡: ç´„{34 * sample_rate:.1f}GB (ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸)")
print(f"å®Ÿéš›ã®RAMä½¿ç”¨é‡: ç´„{34 * sample_rate * 0.3:.1f}GB (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ)")
print("æ¨å¥¨RAM: 32GBä»¥ä¸Š")
print("å¿…è¦æœ€å°RAM: 24GB")
print("")
print("ğŸ”¥ 40GBãƒ‡ãƒ¼ã‚¿å‡¦ç†æ™‚ã®æ¨å¥¨ã‚¹ãƒšãƒƒã‚¯:")
print("- RAM: 64GBä»¥ä¸Š")
print("- GPU: 24GBä»¥ä¸Š (A6000/RTX4090ç­‰)")
print("- SSD: 100GBä»¥ä¸Šã®ç©ºãå®¹é‡") 