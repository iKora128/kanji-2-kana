# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "unsloth",
#     "torch",
#     "transformers",
#     "datasets",
#     "trl",
#     "accelerate",
#     "peft",
#     "bitsandbytes",
#     "pandas",
#     "numpy",
#     "tensorboardx",
# ]
# ///

"""
ğŸš€ Gemma-3-1B SFT L4æœ€é©åŒ–ç‰ˆ - 23GBãƒ¡ãƒ¢ãƒªã‚’æœ€å¤§é™æ´»ç”¨
æ¼¢å­—â†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ã‚¿ã‚¹ã‚¯ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§å¹…æ”¹å–„ã—ãŸå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import torch
import os
import gc
from datasets import Dataset, load_dataset, load_from_disk
from unsloth import FastModel
from unsloth.chat_templates import get_chat_template
from trl import SFTTrainer, SFTConfig

# L4 23GBæœ€å¤§æ´»ç”¨ã®ãŸã‚ã®ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:256,expandable_segments:True"
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# å®šæ•°è¨­å®š
MAX_TRAIN_SAMPLES: int = 1000000   # å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ä¸Šé™ã‚’100ä¸‡ä»¶ã«å¤‰æ›´

# L4æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆ23GBæ´»ç”¨ï¼‰
max_seq_length = 320   # ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ã«æœ€é©ãªé•·ã•ï¼ˆåŠ¹ç‡é‡è¦–ï¼‰
lora_rank = 8          # LoRAãƒ©ãƒ³ã‚¯ã‚’æœ€é©åŒ–ï¼ˆæ¼¢å­—â†’ã‚«ã‚¿ã‚«ãƒŠã«ååˆ†ï¼‰
lora_alpha = 8        # ã‚ˆã‚Šé©åˆ‡ãªalphaå€¤

print("ğŸš€ L4æœ€é©åŒ– Gemma-3-1Bãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
model, tokenizer = FastModel.from_pretrained(
    model_name="unsloth/gemma-3-1b-pt-unsloth-bnb-4bit",
    max_seq_length=max_seq_length,
    load_in_4bit=True,
)

# Gemma-3ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¨­å®š
tokenizer = get_chat_template(
    tokenizer,
    chat_template="gemma-3",
)

# é«˜æ€§èƒ½LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®š
print("âš™ï¸ é«˜æ€§èƒ½LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¨­å®šä¸­...")
model = FastModel.get_peft_model(
    model,
    finetune_vision_layers=False,
    finetune_language_layers=True,
    finetune_attention_modules=True,
    finetune_mlp_modules=True,
    r=lora_rank,
    lora_alpha=lora_alpha,
    lora_dropout=0.01,         
    bias="none",
    random_state=3407,
)

def format_gemma3_conversation(kanji_text: str, katakana_text: str) -> dict:
    """Gemma-3ç”¨ã®ä¼šè©±å½¢å¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    messages = [
        {
            "role": "user",
            "content": f"ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š{kanji_text}"
        },
        {
            "role": "assistant", 
            "content": katakana_text
        }
    ]
    return {"conversations": messages}

def apply_chat_template(examples):
    """ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨"""
    texts = []
    for conversation in examples["conversations"]:
        text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False)
        texts.append(text)
    return {"text": texts}

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
dataset_files = [
    "data/train_llm-jp-corpus-v3.jsonl",
    "data/train_wikipedia.jsonl"
]

print("ğŸ“š datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ï¼†ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¸­...")
cache_dir = "cache/sft_l4_arrow"
if not os.path.exists(cache_dir):
    raw = load_dataset("json", data_files=dataset_files, split="train")
    raw = raw.filter(
        lambda ex: 3 <= len(ex["output"]) <= 80 and 3 <= len(ex["input"]) <= 80 and sum(1 for c in ex["input"] if '\u30A0' <= c <= '\u30FF') / max(len(ex["input"].replace(" ","")),1) >= 0.7,
        num_proc=16
    )
    raw = raw.map(
        lambda examples: {"conversations":[format_gemma3_conversation(o,i)["conversations"] for o,i in zip(examples["output"], examples["input"])]},
        batched=True, batch_size=2000, num_proc=8, remove_columns=["input","output","left_context"]
    )
    raw.save_to_disk(cache_dir)
dataset = load_from_disk(cache_dir)

# å­¦ç¿’ç”¨ã‚µãƒ³ãƒ—ãƒ«ã‚’åˆ¶é™
dataset = dataset.select(range(min(MAX_TRAIN_SAMPLES, len(dataset))))
print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(dataset)} ä»¶")

print("ğŸ”„ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã¨ãƒãƒƒãƒãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä¸¦åˆ—å®Ÿè¡Œä¸­...")
dataset = dataset.map(
    lambda examples: tokenizer(
        [tokenizer.apply_chat_template(conv, tokenize=False) for conv in examples["conversations"]],
        padding="max_length",
        truncation=True,
        max_length=max_seq_length
    ),
    batched=True,
    batch_size=2000,
    num_proc=16,
    remove_columns=["conversations"]
)

# ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª
print("\n=== L4æœ€é©åŒ–ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª ===")
for i in range(min(3, len(dataset))):
    sample = dataset[i]
    # input_ids ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›
    decoded = tokenizer.decode(sample["input_ids"], skip_special_tokens=True)
    print(f"ã‚µãƒ³ãƒ—ãƒ« {i+1}:")
    print(decoded[:250] + "..." if len(decoded) > 250 else decoded)
    print("-" * 50)

# ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
gc.collect()
torch.cuda.empty_cache()

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
if torch.cuda.is_available():
    print(f"ğŸ–¥ï¸ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¾ŒGPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

# L4æœ€é©åŒ–SFTTrainerè¨­å®šï¼ˆ23GBæ´»ç”¨ï¼‰
print("âš™ï¸ L4æœ€å¤§æ´»ç”¨SFTTrainerã‚’è¨­å®šä¸­...")
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    args=SFTConfig(
        dataset_text_field="input_ids",
        per_device_train_batch_size=128,
        gradient_accumulation_steps=4,
        warmup_steps=150,
        num_train_epochs=3,
        learning_rate=5e-4,
        logging_steps=100,
        save_steps=500,
        eval_steps=1000,
        optim = "adamw_8bit",
        weight_decay=0.02,
        lr_scheduler_type="cosine",
        seed=3407,
        output_dir="./outputs/gemma3_1b_kanji2kana_sft_l4_optimized",
        report_to="tensorboard",
        logging_dir="./logs/tensorboard_sft_l4",
        dataloader_pin_memory=True,
        dataloader_num_workers=8,
        gradient_checkpointing=True,  # ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æœ‰åŠ¹åŒ–
        bf16=True,
        fp16=False,
        max_grad_norm=1.0,
        remove_unused_columns=False,
        save_total_limit=20,
    ),
)

print("ğŸš€ L4æœ€å¤§æ´»ç”¨ Gemma-3-1B SFTè¨“ç·´é–‹å§‹...")
print("ğŸ”¥ L4æœ€é©åŒ–è¨­å®š:")
print(f"  - ãƒãƒƒãƒã‚µã‚¤ã‚º: {trainer.args.per_device_train_batch_size}")
print(f"  - å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: {trainer.args.per_device_train_batch_size * trainer.args.gradient_accumulation_steps}")
print(f"  - ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(dataset)}")
print(f"  - LoRAãƒ©ãƒ³ã‚¯: {lora_rank}")
print(f"  - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {max_seq_length}")

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
if torch.cuda.is_available():
    print(f"ğŸ–¥ï¸ è¨“ç·´å‰GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

trainer_stats = trainer.train()

# è¨“ç·´çµ±è¨ˆè¡¨ç¤º
print(f"\nğŸ“Š L4æœ€é©åŒ–SFTè¨“ç·´å®Œäº†!")
print(f"â±ï¸ è¨“ç·´æ™‚é–“: {trainer_stats.metrics['train_runtime']:.2f} ç§’")
print(f"ğŸ“ˆ æœ€çµ‚ãƒ­ã‚¹: {trainer_stats.metrics['train_loss']:.4f}")

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆmodel/é…ä¸‹ã«ä¿å­˜ï¼‰
save_dir = "model/gemma3_1b_kanji2kana_sft_l4_optimized"
print(f"ğŸ’¾ æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ {save_dir} ã«ä¿å­˜ä¸­...")
os.makedirs(save_dir, exist_ok=True)
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

# float16å½¢å¼ã§ã‚‚ä¿å­˜
print("ğŸ’¾ float16å½¢å¼ã§ã‚‚ä¿å­˜ä¸­...")
model.save_pretrained_merged(f"{save_dir}_merged", tokenizer)

# ãƒ†ã‚¹ãƒˆæ¨è«–
print("\n=== ğŸ§ª L4æœ€é©åŒ–SFTè¨“ç·´å¾Œãƒ†ã‚¹ãƒˆæ¨è«– ===")
# æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ
FastModel.for_inference(model)

def test_l4_optimized_inference(text: str) -> str:
    """L4æœ€é©åŒ–SFTè¨“ç·´å¾Œã®ãƒ†ã‚¹ãƒˆæ¨è«–"""
    messages = [{
        "role": "user",
        "content": f"ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š{text}"
    }]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.2,                # ã‚ˆã‚Šæ±ºå®šçš„ãªç”Ÿæˆ
            top_p=0.85,                     # ã‚ˆã‚Šé›†ä¸­ã—ãŸå€™è£œé¸æŠ
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            repetition_penalty=1.05,        # ç¹°ã‚Šè¿”ã—é˜²æ­¢
        )
    
    # å…¥åŠ›éƒ¨åˆ†ã‚’é™¤å»ã—ã¦å‡ºåŠ›ã®ã¿å–å¾—
    generated_tokens = outputs[0][inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    return result.strip()

# æ‹¡å¼µãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
test_cases = [
    "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã€‚",
    "ç§ã¯å­¦ç”Ÿã§ã™ã€‚", 
    "æ±äº¬é§…ã«è¡Œãã¾ã™ã€‚",
    "ç¾ã—ã„èŠ±ãŒå’²ã„ã¦ã„ã¾ã™ã€‚",
    "æ˜æ—¥ã¯é›¨ãŒé™ã‚‹ã§ã—ã‚‡ã†ã€‚",
    "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ã„ã¾ã™ã€‚",
    "æ–°å¹¹ç·šã§å¤§é˜ªã¸è¡Œãã€‚",
    "ç§‘å­¦æŠ€è¡“ã®ç™ºå±•ãŒè‘—ã—ã„ã€‚",
    "å›½éš›ä¼šè­°ãŒé–‹å‚¬ã•ã‚Œã‚‹ã€‚",
    "äººå·¥çŸ¥èƒ½ãŒæ™®åŠã—ã¦ã„ã‚‹ã€‚"
]

print("ğŸ” L4æœ€é©åŒ–SFTè¨“ç·´å¾Œã®æ¨è«–ãƒ†ã‚¹ãƒˆçµæœ:")
for i, test_case in enumerate(test_cases, 1):
    try:
        result = test_l4_optimized_inference(test_case)
        print(f"{i}. å…¥åŠ›: {test_case}")
        print(f"   å‡ºåŠ›: {result}")
        
        # è©³ç´°è©•ä¾¡
        katakana_chars = len([c for c in result if '\u30A0' <= c <= '\u30FF'])
        total_chars = len([c for c in result if not c.isspace()])
        katakana_ratio = katakana_chars / max(total_chars, 1)
        length_ratio = len(result) / len(test_case) if test_case else 0
        
        print(f"   ã‚«ã‚¿ã‚«ãƒŠç‡: {katakana_ratio:.1%}, é•·ã•æ¯”ç‡: {length_ratio:.2f}")
        print("-" * 50)
    except Exception as e:
        print(f"{i}. å…¥åŠ›: {test_case}")
        print(f"   ã‚¨ãƒ©ãƒ¼: {e}")
        print("-" * 50)

# æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
if torch.cuda.is_available():
    print(f"\nğŸ–¥ï¸ æœ€çµ‚GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    print(f"ğŸ“‹ GPUæœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
    print(f"ğŸ“ˆ ãƒ¡ãƒ¢ãƒªæ´»ç”¨ç‡: {torch.cuda.max_memory_allocated() / (23 * 1024**3) * 100:.1f}%")

print(f"\nâœ… L4æœ€é©åŒ– Gemma-3-1B Kanji2Kana SFTè¨“ç·´å®Œäº†ï¼")
print(f"ğŸ’¾ ä¿å­˜å…ˆ: {save_dir}")
print(f"ğŸ“Š ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: {len(dataset)} ã‚µãƒ³ãƒ—ãƒ«")