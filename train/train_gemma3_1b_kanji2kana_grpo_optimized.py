# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "unsloth",
#     "torch",
#     "transformers",
#     "datasets",
#     "trl",
#     "accelerate",
#     "peft",
#     "bitsandbytes",
#     "pandas",
#     "numpy",
#     "tensorboardx",
# ]
# ///

"""
ğŸš€ Gemma-3-1B GRPOæœ€é©åŒ–ç‰ˆ - L4 23GBãƒ¡ãƒ¢ãƒªã‚’æœ€å¤§é™æ´»ç”¨
ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ã‚¿ã‚¹ã‚¯ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§å¹…æ”¹å–„ã—ãŸå¼·åŒ–å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import torch
import os
import re
import gc
from datasets import Dataset
from unsloth import FastModel
from unsloth.chat_templates import get_chat_template
from trl import GRPOConfig, GRPOTrainer

# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:256,expandable_segments:True"
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# L4æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«è¨­å®š
max_seq_length = 256        # ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ã¯çŸ­æ–‡ãŒå¤šã„ãŸã‚çŸ­ç¸®
max_prompt_length = 128     # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã‚‚çŸ­ç¸®
lora_rank = 32              # LoRAãƒ©ãƒ³ã‚¯ã‚’å€å¢—ï¼ˆå“è³ªå‘ä¸Šï¼‰

print("ğŸš€ Gemma-3-1Bæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
model, tokenizer = FastModel.from_pretrained(
    model_name="unsloth/gemma-3-1b-pt-unsloth-bnb-4bit",
    max_seq_length=max_seq_length,
    load_in_4bit=True,
    dtype=torch.bfloat16,  # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªç²¾åº¦
)

# Gemma-3ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¨­å®š
tokenizer = get_chat_template(
    tokenizer,
    chat_template="gemma-3",
)

# é«˜æ€§èƒ½LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®š
print("âš™ï¸ é«˜æ€§èƒ½LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¨­å®šä¸­...")
model = FastModel.get_peft_model(
    model,
    finetune_vision_layers=False,
    finetune_language_layers=True,
    finetune_attention_modules=True,
    finetune_mlp_modules=True,
    r=lora_rank,
    lora_alpha=lora_rank * 2,      # ã‚ˆã‚Šç©æ¥µçš„ãªå­¦ç¿’
    lora_dropout=0.03,             # éå­¦ç¿’é˜²æ­¢ã‚’å°‘ã—ç·©å’Œ
    bias="none",
    random_state=3407,
)

# ==================== æœ€é©åŒ–ã•ã‚ŒãŸå ±é…¬é–¢æ•° ====================

def check_katakana_purity_optimized(completions, **kwargs):
    """
    æœ€é©åŒ–ã•ã‚ŒãŸã‚«ã‚¿ã‚«ãƒŠç´”åº¦ãƒã‚§ãƒƒã‚¯
    ã‚ˆã‚Šç´°ã‹ã„æ®µéšè©•ä¾¡ã¨ãƒœãƒ¼ãƒŠã‚¹åˆ¶åº¦ã‚’è¿½åŠ 
    """
    scores = []
    for completion in completions:
        score = 0
        response = completion[0]["content"].strip()
        
        if not response:
            scores.append(-3.0)  # ç©ºå›ç­”ã®ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’å¼·åŒ–
            continue
            
        # ã‚«ã‚¿ã‚«ãƒŠæ–‡å­—ã®ã¿ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
        katakana_chars = 0
        total_chars = 0
        
        for char in response:
            if char.isspace():
                continue
            total_chars += 1
            if '\u30A0' <= char <= '\u30FF':
                katakana_chars += 1
        
        if total_chars > 0:
            purity = katakana_chars / total_chars
            # ã‚ˆã‚Šç´°ã‹ã„æ®µéšè©•ä¾¡
            if purity == 1.0:       # 100%ã‚«ã‚¿ã‚«ãƒŠï¼ˆå®Œç’§ï¼‰
                score += 4.0
            elif purity >= 0.98:    # 98%ä»¥ä¸Š
                score += 3.5
            elif purity >= 0.95:    # 95%ä»¥ä¸Š
                score += 3.0
            elif purity >= 0.9:     # 90%ä»¥ä¸Š
                score += 2.0
            elif purity >= 0.8:     # 80%ä»¥ä¸Š
                score += 1.0
            elif purity >= 0.6:     # 60%ä»¥ä¸Š
                score += 0.2
            else:
                score -= 2.5  # å¼·åŒ–ã•ã‚ŒãŸãƒšãƒŠãƒ«ãƒ†ã‚£
        else:
            score -= 2.0
            
        scores.append(score)
    return scores

def check_length_appropriateness_optimized(prompts, completions, **kwargs):
    """
    æœ€é©åŒ–ã•ã‚ŒãŸé•·ã•å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
    ã‚ˆã‚Šè©³ç´°ãªç¯„å›²è¨­å®šã¨ãƒœãƒ¼ãƒŠã‚¹åˆ¶åº¦
    """
    scores = []
    for prompt, completion in zip(prompts, completions):
        score = 0
        response = completion[0]["content"].strip()
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰å…ƒã®æ–‡å­—ã‚’æŠ½å‡º
        user_content = prompt[-1]["content"]
        if "ï¼š" in user_content:
            original_text = user_content.split("ï¼š")[-1].strip()
        else:
            original_text = user_content.split(":")[-1].strip()
        
        original_length = len(original_text.replace(" ", ""))
        response_length = len(response.replace(" ", ""))
        
        if original_length > 0:
            ratio = response_length / original_length
            
            # ã‚ˆã‚Šè©³ç´°ãªè©•ä¾¡ç¯„å›²
            if 0.9 <= ratio <= 1.3:    # ç†æƒ³çš„ãªç¯„å›²ï¼ˆ90-130%ï¼‰
                score += 3.0
            elif 0.8 <= ratio <= 1.6:  # è‰¯ã„ç¯„å›²ï¼ˆ80-160%ï¼‰
                score += 2.0
            elif 0.6 <= ratio <= 2.0:  # è¨±å®¹ç¯„å›²ï¼ˆ60-200%ï¼‰
                score += 0.8
            elif 0.4 <= ratio <= 3.0:  # åºƒã„è¨±å®¹ç¯„å›²ï¼ˆ40-300%ï¼‰
                score += 0.2
            elif ratio > 4.0:          # æ¥µç«¯ã«é•·ã„
                score -= 3.0
            elif ratio < 0.3:          # æ¥µç«¯ã«çŸ­ã„
                score -= 2.5
            else:
                score -= 1.5
                
        scores.append(score)
    return scores

def check_unwanted_characters_optimized(completions, **kwargs):
    """
    æœ€é©åŒ–ã•ã‚ŒãŸä¸è¦æ–‡å­—ãƒã‚§ãƒƒã‚¯
    ã‚«ã‚¿ã‚«ãƒŠä¸­ç‚¹ãƒ»é•·éŸ³ç¬¦ã¯è¨±å¯ã€ã‚ˆã‚Šå³æ ¼ãªãƒšãƒŠãƒ«ãƒ†ã‚£
    """
    scores = []
    for completion in completions:
        score = 0
        response = completion[0]["content"].strip()
        
        penalty = 0
        
        # è‹±èªæ–‡å­—ï¼ˆã‚ˆã‚Šå³æ ¼ï¼‰
        english_count = len(re.findall(r'[a-zA-Z]', response))
        penalty += english_count * 0.5
        
        # æ•°å­—
        number_count = len(re.findall(r'[0-9]', response))
        penalty += number_count * 0.4
        
        # ã²ã‚‰ãŒãªï¼ˆå¼·ã„ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
        hiragana_count = len(re.findall(r'[\u3040-\u309F]', response))
        penalty += hiragana_count * 0.8
        
        # æ¼¢å­—ï¼ˆæœ€ã‚‚å¼·ã„ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
        kanji_count = len(re.findall(r'[\u4e00-\u9faf]', response))
        penalty += kanji_count * 1.2
        
        # ç‰¹æ®Šè¨˜å·ï¼ˆã‚«ã‚¿ã‚«ãƒŠä¸­ç‚¹ãƒ»é•·éŸ³ç¬¦ã¯é™¤å¤–ï¼‰
        allowed_symbols = ['ãƒ»', 'ãƒ¼', 'ã€€']  # è¨±å¯ã™ã‚‹è¨˜å·
        for char in response:
            if char in '!@#$%^&*()_+=[]{}|;:<>?/~`"\'\\':
                if char not in allowed_symbols:
                    penalty += 0.3
        
        score -= penalty
        
        # å®Œå…¨ã«ç©ºã®å ´åˆ
        if not response:
            score -= 3.0
            
        scores.append(score)
    return scores

def check_output_completeness_optimized(completions, **kwargs):
    """
    æœ€é©åŒ–ã•ã‚ŒãŸå®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
    ã‚ˆã‚Šç´°ã‹ã„é•·ã•è©•ä¾¡ã¨æ”¹è¡Œãƒ»ç©ºç™½ã®å‡¦ç†
    """
    scores = []
    for completion in completions:
        score = 0
        response = completion[0]["content"].strip()
        
        response_length = len(response)
        
        # ã‚ˆã‚Šç´°ã‹ã„é•·ã•è©•ä¾¡
        if response_length == 0:
            score -= 4.0
        elif 1 <= response_length <= 2:
            score -= 2.0
        elif 3 <= response_length <= 5:
            score -= 0.5
        elif 6 <= response_length <= 80:   # ç†æƒ³çš„ãªç¯„å›²
            score += 2.0
        elif 81 <= response_length <= 120: # å°‘ã—é•·ã„ãŒè¨±å®¹
            score += 1.0
        elif 121 <= response_length <= 200: # é•·ã„
            score -= 0.8
        else:  # æ¥µç«¯ã«é•·ã„
            score -= 2.5
        
        # æ”¹è¡Œæ•°ã®ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šå³æ ¼ï¼‰
        newline_count = response.count('\n')
        if newline_count > 1:
            score -= newline_count * 0.5
            
        # é€£ç¶šç©ºç™½ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
        if '  ' in response:  # é€£ç¶šç©ºç™½
            score -= 0.3
            
        scores.append(score)
    return scores

def check_format_compliance_optimized(completions, **kwargs):
    """
    æœ€é©åŒ–ã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæº–æ‹ ãƒã‚§ãƒƒã‚¯
    ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªä¸è¦ãƒ•ãƒ¬ãƒ¼ã‚ºæ¤œå‡ºã¨ã‚«ã‚¿ã‚«ãƒŠç´”åº¦ãƒœãƒ¼ãƒŠã‚¹
    """
    scores = []
    for completion in completions:
        score = 0
        response = completion[0]["content"].strip()
        
        # æ‹¡å¼µã•ã‚ŒãŸä¸è¦ãƒ•ãƒ¬ãƒ¼ã‚ºãƒªã‚¹ãƒˆ
        unwanted_phrases = [
            "å¤‰æ›ã™ã‚‹ã¨", "ã‚«ã‚¿ã‚«ãƒŠã§", "ç­”ãˆã¯", "çµæœã¯", "ä»¥ä¸‹", "æ¬¡ã®ã‚ˆã†ã«",
            "å¤‰æ›çµæœ", "ç­”ãˆ", "å›ç­”", "ã§ã™", "ã§ã‚ã‚‹", "ã¾ã™", "å¤‰æ›", "ã—ã¦",
            "ã¨ã„ã†", "ã«ãªã‚Šã¾ã™", "ã¯", "ãŒ", "ã‚’", "ã«", "ã®", "ã¨", "ã ",
            "ã‚Œã‚‹", "ã‚‰ã‚Œã‚‹", "ã•ã‚Œã‚‹", "ã§ãã‚‹", "æ€ã†", "è€ƒãˆã‚‹", "ã§ã—ã‚‡ã†"
        ]
        
        penalty = 0
        for phrase in unwanted_phrases:
            penalty += response.count(phrase) * 0.4
        
        score -= penalty
        
        # ã‚«ã‚¿ã‚«ãƒŠç´”åº¦ã«ã‚ˆã‚‹å¤§å¹…ãƒœãƒ¼ãƒŠã‚¹
        if len(response) > 0:
            katakana_chars = len([c for c in response if '\u30A0' <= c <= '\u30FF'])
            total_chars = len([c for c in response if not c.isspace()])
            
            if total_chars > 0:
                purity = katakana_chars / total_chars
                if purity == 1.0:
                    score += 2.5  # å®Œå…¨ã‚«ã‚¿ã‚«ãƒŠãƒœãƒ¼ãƒŠã‚¹
                elif purity >= 0.95:
                    score += 1.8
                elif purity >= 0.9:
                    score += 1.2
                    
        # ç°¡æ½”æ€§ãƒœãƒ¼ãƒŠã‚¹
        if 3 <= len(response) <= 30 and response.count(' ') <= 2:
            score += 0.8
            
        scores.append(score)
    return scores

def check_semantic_appropriateness(prompts, completions, **kwargs):
    """
    æ–°ã—ã„å ±é…¬é–¢æ•°ï¼šæ„å‘³çš„å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
    æ˜ã‚‰ã‹ã«ä¸é©åˆ‡ãªå¤‰æ›ã«ãƒšãƒŠãƒ«ãƒ†ã‚£
    """
    scores = []
    for prompt, completion in zip(prompts, completions):
        score = 0
        response = completion[0]["content"].strip()
        
        # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        user_content = prompt[-1]["content"]
        if "ï¼š" in user_content:
            original_text = user_content.split("ï¼š")[-1].strip()
        else:
            original_text = user_content.split(":")[-1].strip()
        
        # åŸºæœ¬çš„ãªæ„å‘³çš„ãƒã‚§ãƒƒã‚¯
        if response:
            # ç¹°ã‚Šè¿”ã—æ–‡å­—ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
            for char in set(response):
                if response.count(char) > len(original_text):
                    score -= 0.5
            
            # å˜èª¿ãªç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
            if len(set(response.replace(' ', ''))) < 3 and len(response) > 5:
                score -= 1.5
                
            # æ¥µç«¯ã«å˜ç´”ã™ãã‚‹å¤‰æ›ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
            if len(response.replace(' ', '')) < 3 and len(original_text) > 8:
                score -= 1.0
                
        scores.append(score)
    return scores

# ==================== æœ€é©åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ ====================

def format_grpo_prompt(kanji_text: str) -> list[dict]:
    """GRPOç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼"""
    return [
        {
            "role": "user",
            "content": f"ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š{kanji_text}"
        }
    ]

def load_optimized_grpo_dataset(file_paths: list[str], max_samples: int = 25000) -> Dataset:
    """
    æœ€é©åŒ–ã•ã‚ŒãŸGRPOç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    ã‚ˆã‚Šå¤šãã®ã‚µãƒ³ãƒ—ãƒ«ã¨åŠ¹ç‡çš„ãªå‡¦ç†
    """
    data = []
    sample_count = 0
    
    for file_path in file_paths:
        print(f"ğŸ“– å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f):
                    if sample_count >= max_samples:
                        break
                    
                    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ã‚’ä¸Šã’ã¦é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ã‚’å¤šãä½¿ç”¨
                    if torch.rand(1).item() > 0.2:  # 20%ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ4å€å¢—ï¼‰
                        continue
                    
                    try:
                        item = json.loads(line.strip())
                        kanji_text = item["output"]      # æ¼¢å­—æ··ã˜ã‚Šæ–‡ç« 
                        katakana_text = item["input"]    # ã‚«ã‚¿ã‚«ãƒŠæ–‡ç« 
                        
                        # ã‚ˆã‚Šå³æ ¼ãªãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
                        if len(kanji_text) > 40 or len(katakana_text) > 40:
                            continue
                        
                        if len(kanji_text) < 3 or len(katakana_text) < 3:
                            continue
                        
                        if not kanji_text.strip() or not katakana_text.strip():
                            continue
                        
                        # æ–‡å­—ç¨®ã®å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯
                        if len(set(kanji_text)) < 3:  # æ–‡å­—ã®ç¨®é¡ãŒå°‘ãªã™ãã‚‹
                            continue
                            
                        # ã‚«ã‚¿ã‚«ãƒŠç´”åº¦äº‹å‰ãƒã‚§ãƒƒã‚¯
                        katakana_chars = len([c for c in katakana_text if '\u30A0' <= c <= '\u30FF'])
                        total_chars = len([c for c in katakana_text if not c.isspace()])
                        if total_chars > 0 and katakana_chars / total_chars < 0.8:
                            continue
                        
                        prompt_data = {
                            "prompt": format_grpo_prompt(kanji_text),
                            "kanji_text": kanji_text,
                            "expected_katakana": katakana_text,
                            "original_length": len(kanji_text),
                            "target_length": len(katakana_text)
                        }
                        
                        data.append(prompt_data)
                        sample_count += 1
                        
                        if sample_count % 500 == 0:
                            print(f"ğŸ“Š é«˜å“è³ªãƒ‡ãƒ¼ã‚¿å‡¦ç†æ¸ˆã¿: {sample_count}")
                        
                        # ãƒ¡ãƒ¢ãƒªç®¡ç†
                        if sample_count % 5000 == 0:
                            gc.collect()
                    
                    except (json.JSONDecodeError, KeyError) as e:
                        continue
                        
            if sample_count >= max_samples:
                break
                
        except FileNotFoundError:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            continue
    
    print(f"âœ… æœ€é©åŒ–GRPOç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {len(data)} ã‚µãƒ³ãƒ—ãƒ«")
    return Dataset.from_list(data)

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
dataset_files = [
    "data/train_llm-jp-corpus-v3.jsonl",
    "data/train_wikipedia.jsonl"
]

print("ğŸ“š å¤§å®¹é‡æœ€é©åŒ–GRPOç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆä¸­...")
dataset = load_optimized_grpo_dataset(dataset_files, max_samples=25000)

# ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª
print("\n=== ğŸ“‹ æœ€é©åŒ–GRPOç”¨ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª ===")
for i in range(min(3, len(dataset))):
    sample = dataset[i]
    print(f"ã‚µãƒ³ãƒ—ãƒ« {i+1}:")
    print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {sample['prompt']}")
    print(f"æœŸå¾…å‡ºåŠ›: {sample['expected_katakana']}")
    print(f"é•·ã•æ¯”ç‡: {sample['original_length']} â†’ {sample['target_length']}")
    print("-" * 50)

# ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
gc.collect()
torch.cuda.empty_cache()

# ==================== L4æœ€é©åŒ–GRPOè¨“ç·´è¨­å®š ====================

print("âš™ï¸ L4æœ€å¤§æ´»ç”¨GRPOTrainerã‚’è¨­å®šä¸­...")

training_args = GRPOConfig(
    learning_rate=3e-6,                    # å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦å­¦ç¿’ç‡èª¿æ•´
    adam_beta1=0.9,
    adam_beta2=0.99,
    weight_decay=0.05,                     # éå­¦ç¿’é˜²æ­¢ã‚’å¼·åŒ–
    warmup_ratio=0.05,                     # çŸ­ã„ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    lr_scheduler_type="cosine",
    optim="adamw_torch_fused",
    logging_steps=2,                       # ã‚ˆã‚Šé »ç¹ãªãƒ­ã‚°
    per_device_train_batch_size=16,        # å¤§å¹…å¢—åŠ ï¼ˆå…ƒã®8å€ï¼‰
    gradient_accumulation_steps=4,         # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º = 64
    num_generations=8,                     # å€™è£œæ•°ã‚’å€å¢—ï¼ˆå“è³ªå‘ä¸Šï¼‰
    max_prompt_length=max_prompt_length,
    max_completion_length=max_seq_length - max_prompt_length,
    max_steps=1000,                        # ã‚¹ãƒ†ãƒƒãƒ—æ•°å¤§å¹…å¢—åŠ 
    save_steps=100,                        # ã‚ˆã‚Šé »ç¹ãªä¿å­˜
    eval_steps=50,                         # è©•ä¾¡ã‚¹ãƒ†ãƒƒãƒ—è¿½åŠ 
    max_grad_norm=0.5,                     # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç·©å’Œ
    report_to="tensorboard",
    output_dir="outputs/gemma3_1b_kanji2kana_grpo_optimized",
    logging_dir="logs/tensorboard_grpo_optimized",
    dataloader_pin_memory=True,            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æœ€é©åŒ–
    dataloader_num_workers=4,              # ä¸¦åˆ—å‡¦ç†æ•°
    remove_unused_columns=False,           # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿æŒ
    bf16=True,                             # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªè¨ˆç®—
)

trainer = GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    reward_funcs=[
        check_katakana_purity_optimized,        # æœ€é‡è¦ï¼šæœ€é©åŒ–ã•ã‚ŒãŸã‚«ã‚¿ã‚«ãƒŠç´”åº¦
        check_length_appropriateness_optimized, # æœ€é©åŒ–ã•ã‚ŒãŸé•·ã•å¦¥å½“æ€§
        check_unwanted_characters_optimized,    # æœ€é©åŒ–ã•ã‚ŒãŸä¸è¦æ–‡å­—ãƒã‚§ãƒƒã‚¯
        check_output_completeness_optimized,    # æœ€é©åŒ–ã•ã‚ŒãŸå®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        check_format_compliance_optimized,      # æœ€é©åŒ–ã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæº–æ‹ 
        check_semantic_appropriateness,         # æ–°è¦ï¼šæ„å‘³çš„å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
    ],
    args=training_args,
    train_dataset=dataset,
)

print("ğŸš€ L4æœ€å¤§æ´»ç”¨ Gemma-3-1B GRPOè¨“ç·´é–‹å§‹...")
print("ğŸ“Š æœ€é©åŒ–ã•ã‚ŒãŸå ±é…¬é–¢æ•°ï¼ˆ6å€‹ï¼‰:")
print("  1. ã‚«ã‚¿ã‚«ãƒŠç´”åº¦ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰ - 100%ã§+4.0, ã‚ˆã‚Šç´°ã‹ã„æ®µéšè©•ä¾¡")
print("  2. é•·ã•å¦¥å½“æ€§ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰ - 90-130%ã§+3.0, ã‚ˆã‚Šè©³ç´°ãªç¯„å›²")
print("  3. ä¸è¦æ–‡å­—ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰ - ã‚ˆã‚Šå³æ ¼ãªãƒšãƒŠãƒ«ãƒ†ã‚£, ã‚«ã‚¿ã‚«ãƒŠè¨˜å·è¨±å¯")
print("  4. å®Œå…¨æ€§ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰ - ã‚ˆã‚Šç´°ã‹ã„é•·ã•è©•ä¾¡, æ”¹è¡Œãƒ»ç©ºç™½å‡¦ç†")
print("  5. ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæº–æ‹ ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰ - æ‹¡å¼µä¸è¦ãƒ•ãƒ¬ãƒ¼ã‚º, ã‚«ã‚¿ã‚«ãƒŠç´”åº¦ãƒœãƒ¼ãƒŠã‚¹")
print("  6. æ„å‘³çš„å¦¥å½“æ€§ï¼ˆæ–°è¦ï¼‰ - ç¹°ã‚Šè¿”ã—ãƒ»å˜èª¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒšãƒŠãƒ«ãƒ†ã‚£")
print("")
print("ğŸ”¥ L4æœ€é©åŒ–è¨­å®š:")
print(f"  - ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_args.per_device_train_batch_size} (å…ƒã®8å€)")
print(f"  - å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
print(f"  - å€™è£œç”Ÿæˆæ•°: {training_args.num_generations} (å…ƒã®2å€)")
print(f"  - ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(dataset)} (å…ƒã®5å€)")
print(f"  - è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°: {training_args.max_steps} (å…ƒã®5å€)")

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
if torch.cuda.is_available():
    print(f"ğŸ–¥ï¸ è¨“ç·´å‰GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

trainer_stats = trainer.train()

# è¨“ç·´çµ±è¨ˆè¡¨ç¤º
print(f"\nğŸ“Š æœ€é©åŒ–GRPOè¨“ç·´å®Œäº†!")
print(f"â±ï¸ è¨“ç·´æ™‚é–“: {trainer_stats.metrics['train_runtime']:.2f} ç§’")

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
save_dir = "model/gemma3_1b_kanji2kana_grpo_optimized"
print(f"ğŸ’¾ æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ {save_dir} ã«ä¿å­˜ä¸­...")
os.makedirs(save_dir, exist_ok=True)
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

# float16å½¢å¼ã§ã‚‚ä¿å­˜
print("ğŸ’¾ float16å½¢å¼ã§ã‚‚ä¿å­˜ä¸­...")
model.save_pretrained_merged(f"{save_dir}_merged", tokenizer)

# ==================== æœ€é©åŒ–ãƒ†ã‚¹ãƒˆæ¨è«– ====================

print("\n=== ğŸ§ª æœ€é©åŒ–GRPOè¨“ç·´å¾Œãƒ†ã‚¹ãƒˆæ¨è«– ===")
FastModel.for_inference(model)

def test_optimized_grpo_inference(text: str) -> str:
    """æœ€é©åŒ–GRPOè¨“ç·´å¾Œã®ãƒ†ã‚¹ãƒˆæ¨è«–"""
    messages = [{
        "role": "user",
        "content": f"ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š{text}"
    }]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=80,
            temperature=0.2,    # ã‚ˆã‚Šæ±ºå®šçš„ãªç”Ÿæˆ
            top_p=0.85,         # ã‚ˆã‚Šé›†ä¸­ã—ãŸå€™è£œé¸æŠ
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            repetition_penalty=1.1,  # ç¹°ã‚Šè¿”ã—é˜²æ­¢
        )
    
    generated_tokens = outputs[0][inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    return result.strip()

# æ‹¡å¼µãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
test_cases = [
    "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã€‚",
    "ç§ã¯å­¦ç”Ÿã§ã™ã€‚", 
    "æ±äº¬é§…ã«è¡Œãã¾ã™ã€‚",
    "ç¾ã—ã„èŠ±ãŒå’²ã„ã¦ã„ã¾ã™ã€‚",
    "æ˜æ—¥ã¯é›¨ãŒé™ã‚‹ã§ã—ã‚‡ã†ã€‚",
    "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ã„ã¾ã™ã€‚",
    "æ–°å¹¹ç·šã§å¤§é˜ªã¸è¡Œãã€‚",
    "ç§‘å­¦æŠ€è¡“ã®ç™ºå±•ãŒè‘—ã—ã„ã€‚",
    "å›½éš›ä¼šè­°ãŒé–‹å‚¬ã•ã‚Œã‚‹ã€‚",
    "äººå·¥çŸ¥èƒ½ãŒæ™®åŠã—ã¦ã„ã‚‹ã€‚"
]

print("ğŸ” æœ€é©åŒ–GRPOè¨“ç·´å¾Œã®æ¨è«–ãƒ†ã‚¹ãƒˆçµæœ:")
for i, test_case in enumerate(test_cases, 1):
    try:
        result = test_optimized_grpo_inference(test_case)
        print(f"{i}. å…¥åŠ›: {test_case}")
        print(f"   å‡ºåŠ›: {result}")
        
        # è©³ç´°è©•ä¾¡
        katakana_chars = len([c for c in result if '\u30A0' <= c <= '\u30FF'])
        total_chars = len([c for c in result if not c.isspace()])
        katakana_ratio = katakana_chars / max(total_chars, 1)
        length_ratio = len(result) / len(test_case) if test_case else 0
        
        print(f"   ã‚«ã‚¿ã‚«ãƒŠç‡: {katakana_ratio:.1%}, é•·ã•æ¯”ç‡: {length_ratio:.2f}")
        print("-" * 50)
    except Exception as e:
        print(f"{i}. å…¥åŠ›: {test_case}")
        print(f"   ã‚¨ãƒ©ãƒ¼: {e}")
        print("-" * 50)

# æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
if torch.cuda.is_available():
    print(f"\nğŸ–¥ï¸ æœ€çµ‚GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    print(f"ğŸ“‹ GPUæœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
    print(f"ğŸ“ˆ ãƒ¡ãƒ¢ãƒªæ´»ç”¨ç‡: {torch.cuda.max_memory_allocated() / (23 * 1024**3) * 100:.1f}%")

print(f"\nâœ… L4æœ€é©åŒ– Gemma-3-1B Kanji2Kana GRPOè¨“ç·´å®Œäº†ï¼")
print(f"ğŸ’¾ ä¿å­˜å…ˆ: {save_dir}")
print(f"ğŸ“Š ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: {len(dataset)} ã‚µãƒ³ãƒ—ãƒ«")

print("\n" + "="*70)
print("ğŸš€ L4æœ€é©åŒ–GRPO ã®è©³ç´°æ”¹å–„ç‚¹:")
print("="*70)
print("ã€ãƒ¡ãƒ¢ãƒªæ´»ç”¨ã€‘")
print("- ãƒãƒƒãƒã‚µã‚¤ã‚º: 2 â†’ 16 (8å€å¢—)")
print("- å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: 4 â†’ 64 (16å€å¢—)")
print("- ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: 512 â†’ 256 (åŠ¹ç‡åŒ–)")
print("- LoRAãƒ©ãƒ³ã‚¯: 16 â†’ 32 (å“è³ªå‘ä¸Š)")
print("")
print("ã€ãƒ‡ãƒ¼ã‚¿å“è³ªã€‘")
print("- ã‚µãƒ³ãƒ—ãƒ«æ•°: 5,000 â†’ 25,000 (5å€å¢—)")
print("- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: 5% â†’ 20% (4å€å¢—)")
print("- å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¼·åŒ–")
print("")
print("ã€å ±é…¬é–¢æ•°å¼·åŒ–ã€‘")
print("- 6ã¤ã®æœ€é©åŒ–ã•ã‚ŒãŸå ±é…¬é–¢æ•°")
print("- ã‚ˆã‚Šç´°ã‹ã„æ®µéšè©•ä¾¡")
print("- æ„å‘³çš„å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯è¿½åŠ ")
print("- ã‚«ã‚¿ã‚«ãƒŠè¨˜å·(ãƒ»ã€ãƒ¼)è¨±å¯")
print("")
print("ã€å­¦ç¿’åŠ¹ç‡ã€‘")
print("- å€™è£œç”Ÿæˆæ•°: 4 â†’ 8 (2å€å¢—)")
print("- è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—: 200 â†’ 1,000 (5å€å¢—)")
print("- ç¹°ã‚Šè¿”ã—é˜²æ­¢å¼·åŒ–")
print("")
print("ğŸ¯ æœŸå¾…ã•ã‚Œã‚‹å¤§å¹…æ”¹å–„:")
print("- å­¦ç¿’é€Ÿåº¦: 16å€é«˜é€ŸåŒ–")
print("- GPUãƒ¡ãƒ¢ãƒªæ´»ç”¨ç‡: 10% â†’ 70-80%")
print("- ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ç²¾åº¦ã®å¤§å¹…å‘ä¸Š")
print("- ã‚ˆã‚Šè‡ªç„¶ã§é©åˆ‡ãªé•·ã•ã®å‡ºåŠ›")
print("")
print("âš¡ å°†æ¥ã®40GBãƒ‡ãƒ¼ã‚¿å¯¾å¿œæº–å‚™å®Œäº†ï¼") 