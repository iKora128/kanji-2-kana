# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "unsloth",
#     "torch",
#     "transformers",
#     "datasets",
#     "trl",
#     "accelerate",
#     "peft",
#     "bitsandbytes",
#     "pandas",
#     "numpy",
# ]
# ///

"""
Qwen1.7Bã§SFTTrainerã‚’ä½¿ã£ãŸæ¼¢å­—â†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆSeq2Seqãƒ‡ãƒ¼ã‚¿å½¢å¼ï¼‰
"""

import json
import torch
from datasets import Dataset
from unsloth import FastLanguageModel
from trl import SFTTrainer, SFTConfig

# ãƒ¢ãƒ‡ãƒ«è¨­å®š
max_seq_length = 256  # Seq2Seqã§ã¯çŸ­ã‚ã«è¨­å®š
lora_rank = 16

print("ğŸš€ ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen3-1.7B-unsloth-bnb-4bit",
    max_seq_length=max_seq_length,
    load_in_4bit=True,
    fast_inference=False,  # vLLMã‚’ä½¿ã‚ãªã„
    max_lora_rank=lora_rank,
)

# Seq2Seqã®ãŸã‚ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼è¨­å®š
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¿½åŠ 
model = FastLanguageModel.get_peft_model(
    model,
    r=lora_rank,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
    lora_alpha=lora_rank * 2,
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)

def format_seq2seq_prompt(kanji_text: str, katakana_text: str = "") -> str:
    """Seq2Seqå½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ"""
    if katakana_text:
        # è¨“ç·´æ™‚
        return f"æ¼¢å­—ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚\nå…¥åŠ›: {kanji_text}\nå‡ºåŠ›: {katakana_text}"
    else:
        # æ¨è«–æ™‚
        return f"æ¼¢å­—ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚\nå…¥åŠ›: {kanji_text}\nå‡ºåŠ›: "

def load_seq2seq_dataset(file_paths: list[str], max_samples: int = 5000) -> Dataset:
    """Seq2Seqç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    data = []
    sample_count = 0
    
    for file_path in file_paths:
        print(f"ğŸ“– èª­ã¿è¾¼ã¿é–‹å§‹: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f):
                    if sample_count >= max_samples:
                        print(f"âœ… æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•° {max_samples} ã«åˆ°é”")
                        break
                    
                    try:
                        item = json.loads(line.strip())
                        kanji_text = item["output"]      # æ¼¢å­—æ··ã˜ã‚Šæ–‡ç« 
                        katakana_text = item["input"]    # ã‚«ã‚¿ã‚«ãƒŠæ–‡ç« 
                        
                        # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
                        if len(kanji_text) > 80 or len(katakana_text) > 80:
                            continue
                        
                        # ç©ºæ–‡å­—åˆ—ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        if not kanji_text.strip() or not katakana_text.strip():
                            continue
                        
                        # Seq2Seqå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                        text = format_seq2seq_prompt(kanji_text, katakana_text)
                        
                        # ãƒˆãƒ¼ã‚¯ãƒ³é•·ãƒã‚§ãƒƒã‚¯
                        if len(tokenizer.encode(text)) <= max_seq_length:
                            data.append({"text": text})
                            sample_count += 1
                            
                            if sample_count % 1000 == 0:
                                print(f"ğŸ“Š å‡¦ç†æ¸ˆã¿: {sample_count}")
                    
                    except (json.JSONDecodeError, KeyError):
                        continue
                    except Exception as e:
                        if line_num % 10000 == 0:
                            print(f"âš ï¸ è¡Œ {line_num} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                        
                if sample_count >= max_samples:
                    break
                    
        except FileNotFoundError:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            continue
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {len(data)} ã‚µãƒ³ãƒ—ãƒ«")
    return Dataset.from_list(data)

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
dataset_files = [
    "data/train_llm-jp-corpus-v3.jsonl",
    "data/train_wikipedia.jsonl"
]

print("ğŸ“š Seq2Seqãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆä¸­...")
dataset = load_seq2seq_dataset(dataset_files, max_samples=5000)

# ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª
print("\n=== ğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª ===")
for i in range(2):
    sample = dataset[i]
    print(f"ã‚µãƒ³ãƒ—ãƒ« {i+1}:")
    print(sample["text"])
    print("-" * 50)

# SFTTrainerè¨­å®šï¼ˆSeq2Seqãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
print("âš™ï¸ SFTTrainerã‚’è¨­å®šä¸­...")
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    args=SFTConfig(
        dataset_text_field="text",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=2,
        warmup_steps=100,
        num_train_epochs=10,
        learning_rate=2e-4,
        logging_steps=50,
        save_steps=200,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="./outputs_seq2seq_sft",
        report_to="none",
    ),
)

print("ğŸš€ Seq2Seq SFTè¨“ç·´é–‹å§‹...")
trainer.train()

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...")
model.save_pretrained("kanji2kana_seq2seq_sft_lora")
tokenizer.save_pretrained("kanji2kana_seq2seq_sft_lora")

# ãƒ†ã‚¹ãƒˆæ¨è«–
print("\n=== ğŸ§ª ãƒ†ã‚¹ãƒˆæ¨è«– ===")
# æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ
FastLanguageModel.for_inference(model)

def test_inference(text: str) -> str:
    """ãƒ†ã‚¹ãƒˆæ¨è«–é–¢æ•°"""
    prompt = format_seq2seq_prompt(text)
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        max_length=max_seq_length,
        truncation=True,
        padding=True
    ).to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # å…¥åŠ›éƒ¨åˆ†ã‚’é™¤å»ã—ã¦å‡ºåŠ›ã®ã¿å–å¾—
    generated_tokens = outputs[0][inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    return result.strip()

# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
test_cases = [
    "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã€‚",
    "ç§ã¯å­¦ç”Ÿã§ã™ã€‚",
    "æ±äº¬é§…ã«è¡Œãã¾ã™ã€‚",
    "ç¾ã—ã„èŠ±ãŒå’²ã„ã¦ã„ã¾ã™ã€‚",
    "æ˜æ—¥ã¯é›¨ãŒé™ã‚‹ã§ã—ã‚‡ã†ã€‚"
]

print("ğŸ” æ¨è«–ãƒ†ã‚¹ãƒˆçµæœ:")
for i, test_case in enumerate(test_cases, 1):
    result = test_inference(test_case)
    print(f"{i}. å…¥åŠ›: {test_case}")
    print(f"   å‡ºåŠ›: {result}")
    print("-" * 40)

print("\nâœ… Seq2Seq SFTè¨“ç·´å®Œäº†ï¼")

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
if torch.cuda.is_available():
    print(f"ğŸ–¥ï¸ GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB") 