# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "unsloth",
#     "torch",
#     "transformers",
#     "vllm",
# ]
# ///

"""
ğŸš€ è¶…é«˜é€Ÿæ¼¢å­—â†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æœ€é©åŒ–ã•ã‚ŒãŸç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é«˜é€Ÿæ¨è«–
"""

import torch
import time
from unsloth import FastModel
from unsloth.chat_templates import get_chat_template
import sys
import os
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from vllm import LLM

class FastKanji2Katakana:
    """é«˜é€Ÿæ¼¢å­—â†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_path: str = "model/gemma3_1b_kanji2kana_sft_merged"):
        self.model = None
        self.tokenizer = None
        self.model_path = model_path
        
    def load_model(self):
        """é«˜é€ŸåŒ–è¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        print(f"ğŸš€ é«˜é€Ÿæ¨è«–ç”¨ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {self.model_path}")
        
        start_time = time.time()
        
        # é«˜é€ŸåŒ–è¨­å®š
        self.model, self.tokenizer = FastModel.from_pretrained(
            model_name=self.model_path,
            max_seq_length=256,  # çŸ­ç¸®ï¼ˆã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ã«ååˆ†ï¼‰
            load_in_4bit=True,
            dtype=torch.bfloat16,
        )
        
        self.tokenizer = get_chat_template(self.tokenizer, chat_template="gemma-3")
        FastModel.for_inference(self.model)
        
        # ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆPyTorch 2.0+ã§é«˜é€ŸåŒ–ï¼‰
        if hasattr(torch, 'compile'):
            print("ğŸ”¥ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ä¸­ï¼ˆåˆå›ã®ã¿æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰...")
            self.model = torch.compile(self.model, mode="reduce-overhead")
        
        load_time = time.time() - start_time
        print(f"âœ… é«˜é€Ÿãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼ ({load_time:.2f}ç§’)")
        
        return self
    
    def convert(self, text: str) -> tuple[str, float]:
        """è¶…é«˜é€Ÿå¤‰æ›"""
        if self.model is None:
            raise RuntimeError("ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚load_model()ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        
        start_time = time.time()
        
        messages = [{
            "role": "user",
            "content": f"ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š{text}"
        }]
        
        prompt = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=50,          # å¤§å¹…çŸ­ç¸®ï¼ˆã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ã«ååˆ†ï¼‰
                temperature=0.0,            # æ±ºå®šçš„ç”Ÿæˆï¼ˆæœ€é«˜é€Ÿï¼‰
                do_sample=False,            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç„¡åŠ¹åŒ–ï¼ˆé«˜é€ŸåŒ–ï¼‰
                pad_token_id=self.tokenizer.pad_token_id,
                use_cache=True,             # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨
            )
        
        generated_tokens = outputs[0][inputs["input_ids"].shape[1]:]
        result = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        inference_time = time.time() - start_time
        return result.strip(), inference_time
    
    def batch_convert(self, texts: list[str]) -> list[tuple[str, float]]:
        """ãƒãƒƒãƒå‡¦ç†ã§æ›´ãªã‚‹é«˜é€ŸåŒ–"""
        results = []
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´ï¼ˆGPU ãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦ï¼‰
        batch_size = min(8, len(texts))
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_start = time.time()
            
            # ãƒãƒƒãƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            batch_messages = []
            for text in batch_texts:
                messages = [{
                    "role": "user",
                    "content": f"ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š{text}"
                }]
                batch_messages.append(messages)
            
            # ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            batch_prompts = [
                self.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
                for msgs in batch_messages
            ]
            
            batch_inputs = self.tokenizer(
                batch_prompts, 
                return_tensors="pt", 
                padding=True,
                truncation=True,
                max_length=256
            ).to("cuda")
            
            with torch.no_grad():
                batch_outputs = self.model.generate(
                    **batch_inputs,
                    max_new_tokens=50,
                    temperature=0.0,
                    do_sample=False,
                    pad_token_id=self.tokenizer.pad_token_id,
                    use_cache=True,
                )
            
            batch_time = time.time() - batch_start
            
            # çµæœã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
            for j, (text, output) in enumerate(zip(batch_texts, batch_outputs)):
                input_length = len(batch_inputs["input_ids"][j])
                generated_tokens = output[input_length:]
                result = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                
                # å€‹åˆ¥ã®æ¨è«–æ™‚é–“ã‚’æ¨å®š
                individual_time = batch_time / len(batch_texts)
                results.append((result.strip(), individual_time))
        
        return results

def analyze_text_fast(text: str) -> dict:
    """é«˜é€Ÿæ–‡å­—ç¨®åˆ†æ"""
    katakana = sum(1 for c in text if '\u30A0' <= c <= '\u30FF')
    hiragana = sum(1 for c in text if '\u3040' <= c <= '\u309F')
    kanji = sum(1 for c in text if '\u4E00' <= c <= '\u9FAF')
    japanese_total = katakana + hiragana + kanji + sum(1 for c in text if c.isalnum())
    
    return {
        'katakana': katakana,
        'hiragana': hiragana,
        'kanji': kanji,
        'japanese_total': japanese_total,
        'katakana_ratio': katakana / max(japanese_total, 1)
    }

def interactive_fast_mode():
    """é«˜é€Ÿå¯¾è©±ãƒ¢ãƒ¼ãƒ‰"""
    print("ğŸš€ è¶…é«˜é€Ÿæ¼¢å­—â†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ã‚·ã‚¹ãƒ†ãƒ ")
    print("="*60)
    
    converter = FastKanji2Katakana().load_model()
    
    # GPUä½¿ç”¨é‡è¡¨ç¤º
    if torch.cuda.is_available():
        print(f"ğŸ–¥ï¸ GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    
    print("\nğŸ’¡ è¶…é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰è¨­å®š:")
    print("  - æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: 50 (é€šå¸¸ã®åŠåˆ†)")
    print("  - æ±ºå®šçš„ç”Ÿæˆ: temperature=0.0")
    print("  - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç„¡åŠ¹: do_sample=False")
    print("  - ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–: æœ‰åŠ¹")
    print("  - ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«: æœ‰åŠ¹")
    
    print("\n" + "="*60)
    print("ğŸŒ è¶…é«˜é€Ÿæ¼¢å­—â†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›")
    print("="*60)
    print("ğŸ’¡ 'batch' ã§ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰, 'quit' ã§çµ‚äº†")
    print("-" * 60)
    
    while True:
        try:
            user_input = input("\nğŸ“ å¤‰æ›ã—ãŸã„æ–‡ç« : ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ çµ‚äº†ã—ã¾ã™")
                break
            
            if user_input.lower() == 'batch':
                batch_mode(converter)
                continue
            
            if not user_input:
                continue
            
            # è¶…é«˜é€Ÿå¤‰æ›
            result, inference_time = converter.convert(user_input)
            analysis = analyze_text_fast(result)
            
            print(f"ğŸ“¥ å‡ºåŠ›: {result}")
            print(f"âš¡ æ¨è«–æ™‚é–“: {inference_time:.3f}ç§’")
            print(f"ğŸš€ å‡¦ç†é€Ÿåº¦: {len(user_input)/inference_time:.1f} æ–‡å­—/ç§’")
            print(f"ğŸ“Š ã‚«ã‚¿ã‚«ãƒŠç‡: {analysis['katakana_ratio']:.1%} ({analysis['katakana']}/{analysis['japanese_total']}æ–‡å­—)")
            
            if analysis['hiragana'] > 0:
                print(f"âš ï¸ ã²ã‚‰ãŒãª: {analysis['hiragana']}æ–‡å­—")
            if analysis['kanji'] > 0:
                print(f"âš ï¸ æ¼¢å­—: {analysis['kanji']}æ–‡å­—")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ çµ‚äº†ã—ã¾ã™")
            break
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

def batch_mode(converter):
    """ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰"""
    print("\nğŸ“¦ è¶…é«˜é€Ÿãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰")
    print("="*40)
    print("è¤‡æ•°ã®æ–‡ç« ã‚’ä¸€åº¦ã«é«˜é€Ÿå‡¦ç†ã—ã¾ã™")
    print("ç©ºè¡Œã§å‡¦ç†é–‹å§‹ã€'back'ã§æˆ»ã‚‹")
    print("-" * 40)
    
    texts = []
    while True:
        try:
            line = input().strip()
            if line.lower() == 'back':
                return
            if not line:
                if texts:
                    break
                continue
            texts.append(line)
            print(f"  {len(texts)}. {line}")
        except KeyboardInterrupt:
            return
    
    print(f"\nğŸš€ {len(texts)}ä»¶ã‚’ãƒãƒƒãƒå‡¦ç†ä¸­...")
    
    start_time = time.time()
    results = converter.batch_convert(texts)
    total_time = time.time() - start_time
    
    total_chars = sum(len(text) for text in texts)
    
    print(f"\nğŸ“Š ãƒãƒƒãƒå‡¦ç†çµæœ (ç·æ™‚é–“: {total_time:.3f}ç§’)")
    print(f"ğŸš€ ç·åˆå‡¦ç†é€Ÿåº¦: {total_chars/total_time:.1f} æ–‡å­—/ç§’")
    print("="*50)
    
    for i, (text, (result, individual_time)) in enumerate(zip(texts, results), 1):
        analysis = analyze_text_fast(result)
        print(f"\n{i}. å…¥åŠ›: {text}")
        print(f"   å‡ºåŠ›: {result}")
        print(f"   æ™‚é–“: {individual_time:.3f}ç§’ | é€Ÿåº¦: {len(text)/individual_time:.1f} æ–‡å­—/ç§’")
        print(f"   ã‚«ã‚¿ã‚«ãƒŠç‡: {analysis['katakana_ratio']:.1%}")

def speed_benchmark():
    """æ¨è«–é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print("ğŸ æ¨è«–é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("="*50)
    
    converter = FastKanji2Katakana().load_model()
    
    test_texts = [
        "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã€‚",
        "ç§ã¯æ±äº¬é§…ã«è¡Œãã¾ã™ã€‚",
        "ç¾ã—ã„èŠ±ãŒå’²ã„ã¦ã„ã¾ã™ã€‚",
        "ç§‘å­¦æŠ€è¡“ã®ç™ºå±•ãŒè‘—ã—ã„ã€‚",
        "äººå·¥çŸ¥èƒ½ãŒæ™®åŠã—ã¦ã„ã‚‹ã€‚",
        "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ã£ã¦ç ”ç©¶ã‚’è¡Œã†ã€‚",
        "æ–°å¹¹ç·šã§å¤§é˜ªã‹ã‚‰æ±äº¬ã¾ã§ç§»å‹•ã—ãŸã€‚",
        "å›½éš›ä¼šè­°ãŒæ¥å¹´ã®æ˜¥ã«é–‹å‚¬ã•ã‚Œã‚‹äºˆå®šã§ã™ã€‚",
        "æ©Ÿæ¢°å­¦ç¿’ã¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®æŠ€è¡“ãŒæ€¥é€Ÿã«é€²æ­©ã—ã¦ã„ã‚‹ã€‚",
        "è‡ªç„¶è¨€èªå‡¦ç†ã®åˆ†é‡ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ãŒæ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ã€‚"
    ]
    
    print(f"ğŸ“ {len(test_texts)}ä»¶ã®æ–‡ç« ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ...")
    
    # å€‹åˆ¥å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    print("\nğŸ”¥ å€‹åˆ¥å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯:")
    individual_times = []
    total_chars = 0
    
    for i, text in enumerate(test_texts, 1):
        result, inference_time = converter.convert(text)
        individual_times.append(inference_time)
        total_chars += len(text)
        
        print(f"{i:2d}. {len(text):2d}æ–‡å­— | {inference_time:.3f}ç§’ | {len(text)/inference_time:5.1f} æ–‡å­—/ç§’ | {text[:20]}...")
    
    print(f"\nğŸ“Š å€‹åˆ¥å‡¦ç†çµ±è¨ˆ:")
    print(f"å¹³å‡æ™‚é–“: {sum(individual_times)/len(individual_times):.3f}ç§’")
    print(f"ç·åˆé€Ÿåº¦: {total_chars/sum(individual_times):.1f} æ–‡å­—/ç§’")
    
    # ãƒãƒƒãƒå‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    print(f"\nğŸš€ ãƒãƒƒãƒå‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯:")
    batch_start = time.time()
    batch_results = converter.batch_convert(test_texts)
    batch_total_time = time.time() - batch_start
    
    print(f"ãƒãƒƒãƒç·æ™‚é–“: {batch_total_time:.3f}ç§’")
    print(f"ãƒãƒƒãƒç·åˆé€Ÿåº¦: {total_chars/batch_total_time:.1f} æ–‡å­—/ç§’")
    print(f"é€Ÿåº¦å‘ä¸Šç‡: {(total_chars/batch_total_time) / (total_chars/sum(individual_times)):.1f}x")

def hf_pipeline_mode():
    """Transformers pipelineã«ã‚ˆã‚‹é«˜é€Ÿæ¨è«–ãƒ¢ãƒ¼ãƒ‰"""
    print("ğŸš€ HF pipelineã«ã‚ˆã‚‹æ¨è«–ãƒ¢ãƒ¼ãƒ‰")
    tokenizer = AutoTokenizer.from_pretrained(
        "model/gemma3_1b_kanji2kana_sft_merged", use_fast=True
    )
    model = AutoModelForCausalLM.from_pretrained(
        "model/gemma3_1b_kanji2kana_sft_merged",
        device_map="auto",
        load_in_8bit=True,
        torch_dtype=torch.float16,
    )
    generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=30,
        do_sample=False,
    )
    while True:
        text = input("ğŸ“ æ–‡ç« ã‚’å…¥åŠ› (quitã§çµ‚äº†): ").strip()
        if text.lower() in ['quit','exit','q']:
            break
        prompt = f"ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š{text}"
        res = generator(prompt)[0]["generated_text"]
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’å‰Šé™¤
        output = res.split(prompt)[-1]
        print(f"ğŸ“¥ å‡ºåŠ›: {output.strip()}")

def vllm_mode():
    """vLLMã«ã‚ˆã‚‹é«˜é€Ÿæ¨è«–ãƒ¢ãƒ¼ãƒ‰"""
    print("ğŸš€ vLLMã«ã‚ˆã‚‹æ¨è«–ãƒ¢ãƒ¼ãƒ‰")
    llm = LLM(
        model="model/gemma3_1b_kanji2kana_sft_merged",
        dtype="bfloat16",
        tensor_parallel_size=1
    )
    while True:
        text = input("ğŸ“ æ–‡ç« ã‚’å…¥åŠ› (quitã§çµ‚äº†): ").strip()
        if text.lower() in ['quit','exit','q']:
            break
        prompt = f"ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š{text}"
        outputs = llm.generate([prompt], max_tokens=30)
        result = outputs[0].text
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’å‰Šé™¤
        output = result.split(prompt)[-1]
        print(f"ğŸ“¥ å‡ºåŠ›: {output.strip()}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    if len(sys.argv) > 1:
        if sys.argv[1] == '--benchmark':
            speed_benchmark()
        elif sys.argv[1] == '--hf':
            hf_pipeline_mode()
        elif sys.argv[1] == '--vllm':
            vllm_mode()
        elif sys.argv[1] in ['--help','-h']:
            print("ä½¿ã„æ–¹:")
            print("  python fast_inference.py                # è¶…é«˜é€Ÿå¯¾è©±ãƒ¢ãƒ¼ãƒ‰")
            print("  python fast_inference.py --benchmark    # é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
            print("  python fast_inference.py --hf           # Transformers pipelineãƒ¢ãƒ¼ãƒ‰")
            print("  python fast_inference.py --vllm         # vLLMãƒ¢ãƒ¼ãƒ‰")
        else:
            print("ä¸æ˜ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³:", sys.argv[1])
    else:
        interactive_fast_mode()

if __name__ == "__main__":
    main() 