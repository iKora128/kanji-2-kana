# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "unsloth",
#     "torch",
#     "transformers",
# ]
# ///

"""
ğŸš€ Gemma-3-1Bæ¼¢å­—â†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å­¦ç¿’æ¸ˆã¿SFTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿæ¨è«–
"""

import torch
from unsloth import FastModel
from unsloth.chat_templates import get_chat_template
import sys
import os
import time

def load_trained_model(model_path: str = "model/gemma3_1b_kanji2kana_sft_merged"):
    """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆmergedãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§é«˜é€Ÿæ¨è«–ï¼‰"""
    print(f"ğŸš€ æœ€é©åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_path}")
    
    try:
        # mergedãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆæ¨è«–æœ€é©åŒ–æ¸ˆã¿ï¼‰
        model, tokenizer = FastModel.from_pretrained(
            model_name=model_path,
            max_seq_length=512,
            load_in_4bit=True,
            dtype=torch.bfloat16,
        )
        
        # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¨­å®š
        tokenizer = get_chat_template(
            tokenizer,
            chat_template="gemma-3",
        )
        
        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
        FastModel.for_inference(model)
        
        print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼")
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ğŸ’¡ ãƒ‘ã‚¹ '{model_path}' ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        return None, None

def kanji_to_katakana(model, tokenizer, text: str) -> tuple[str, float]:
    """æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ï¼ˆæ¨è«–æ™‚é–“ã‚‚æ¸¬å®šï¼‰"""
    start_time = time.time()
    
    messages = [{
        "role": "user",
        "content": f"ä»¥ä¸‹ã®æ¼¢å­—æ··ã˜ã‚Šæ–‡ã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š{text}"
    }]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.1,                # æ±ºå®šçš„ãªç”Ÿæˆ
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            repetition_penalty=1.05,
        )
    
    # å…¥åŠ›éƒ¨åˆ†ã‚’é™¤å»ã—ã¦å‡ºåŠ›ã®ã¿å–å¾—
    generated_tokens = outputs[0][inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    inference_time = time.time() - start_time
    return result.strip(), inference_time

def interactive_mode(model, tokenizer):
    """å¯¾è©±ãƒ¢ãƒ¼ãƒ‰"""
    print("\n" + "="*60)
    print("ğŸŒ æ¼¢å­—â†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ› å¯¾è©±ãƒ¢ãƒ¼ãƒ‰")
    print("="*60)
    print("ğŸ’¡ ä½¿ã„æ–¹:")
    print("  - æ¼¢å­—æ··ã˜ã‚Šã®æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    print("  - 'quit' ã¾ãŸã¯ 'exit' ã§çµ‚äº†")
    print("  - 'batch' ã§ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ")
    print("-" * 60)
    
    while True:
        try:
            user_input = input("\nğŸ“ å¤‰æ›ã—ãŸã„æ–‡ç« ã‚’å…¥åŠ›: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ æ¨è«–ã‚’çµ‚äº†ã—ã¾ã™")
                break
            
            if user_input.lower() == 'batch':
                batch_mode(model, tokenizer)
                continue
            
            if not user_input:
                print("âš ï¸ æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                continue
            
            # å¤‰æ›å®Ÿè¡Œ
            print("ğŸ”„ å¤‰æ›ä¸­...")
            result, inference_time = kanji_to_katakana(model, tokenizer, user_input)
            
            print(f"ğŸ“¤ å…¥åŠ›: {user_input}")
            print(f"ğŸ“¥ å‡ºåŠ›: {result}")
            print(f"â±ï¸ æ¨è«–æ™‚é–“: {inference_time:.3f}ç§’")
            
            # è©³ç´°è©•ä¾¡ï¼ˆå¥èª­ç‚¹ãƒ»è¨˜å·ã‚’é™¤å¤–ï¼‰
            katakana_chars = len([c for c in result if '\u30A0' <= c <= '\u30FF'])
            # æ—¥æœ¬èªæ–‡å­—ã®ã¿ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã€è‹±æ•°å­—ï¼‰
            japanese_chars = len([c for c in result if (
                '\u3040' <= c <= '\u309F' or  # ã²ã‚‰ãŒãª
                '\u30A0' <= c <= '\u30FF' or  # ã‚«ã‚¿ã‚«ãƒŠ
                '\u4E00' <= c <= '\u9FAF' or  # æ¼¢å­—
                c.isalnum()                   # è‹±æ•°å­—
            )])
            katakana_ratio = katakana_chars / max(japanese_chars, 1)
            
            print(f"ğŸ“Š ã‚«ã‚¿ã‚«ãƒŠç‡: {katakana_ratio:.1%}")
            print(f"ğŸš€ æ¨è«–é€Ÿåº¦: {len(user_input)/inference_time:.1f} æ–‡å­—/ç§’")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ æ¨è«–ã‚’çµ‚äº†ã—ã¾ã™")
            break
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

def batch_mode(model, tokenizer):
    """ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰"""
    print("\n" + "="*60)
    print("ğŸ“¦ ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰")
    print("="*60)
    print("ğŸ’¡ è¤‡æ•°ã®æ–‡ç« ã‚’ä¸€åº¦ã«å‡¦ç†ã—ã¾ã™")
    print("  - å„è¡Œã«1ã¤ã®æ–‡ç« ã‚’å…¥åŠ›")
    print("  - ç©ºè¡Œã§å‡¦ç†é–‹å§‹")
    print("  - 'back' ã§å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ã«æˆ»ã‚‹")
    print("-" * 60)
    
    texts = []
    print("ğŸ“ å¤‰æ›ã—ãŸã„æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (ç©ºè¡Œã§å‡¦ç†é–‹å§‹):")
    
    while True:
        try:
            line = input().strip()
            
            if line.lower() == 'back':
                return
            
            if not line:  # ç©ºè¡Œã§å‡¦ç†é–‹å§‹
                if texts:
                    break
                else:
                    print("âš ï¸ æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                    continue
            
            texts.append(line)
            print(f"  {len(texts)}. {line}")
            
        except KeyboardInterrupt:
            return
    
    # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
    print(f"\nğŸ”„ {len(texts)}ä»¶ã®æ–‡ç« ã‚’å¤‰æ›ä¸­...")
    results = []
    
    total_time = 0
    for i, text in enumerate(texts, 1):
        try:
            result, inference_time = kanji_to_katakana(model, tokenizer, text)
            total_time += inference_time
            results.append((text, result, inference_time))
            print(f"âœ… {i}/{len(texts)} å®Œäº† ({inference_time:.3f}ç§’)")
        except Exception as e:
            results.append((text, f"ã‚¨ãƒ©ãƒ¼: {e}", 0))
            print(f"âŒ {i}/{len(texts)} ã‚¨ãƒ©ãƒ¼")
    
    # çµæœè¡¨ç¤º
    print("\n" + "="*60)
    print("ğŸ“‹ ãƒãƒƒãƒå‡¦ç†çµæœ")
    print("="*60)
    print(f"ğŸ“Š ç·å‡¦ç†æ™‚é–“: {total_time:.3f}ç§’")
    print(f"âš¡ å¹³å‡å‡¦ç†æ™‚é–“: {total_time/len(texts):.3f}ç§’/æ–‡")
    print(f"ğŸš€ å¹³å‡å‡¦ç†é€Ÿåº¦: {sum(len(t) for t, _, _ in results)/total_time:.1f} æ–‡å­—/ç§’")
    
    for i, (input_text, output_text, inference_time) in enumerate(results, 1):
        print(f"\n{i}. å…¥åŠ›: {input_text}")
        print(f"   å‡ºåŠ›: {output_text}")
        if inference_time > 0:
            print(f"   æ¨è«–æ™‚é–“: {inference_time:.3f}ç§’")
        
        if not output_text.startswith("ã‚¨ãƒ©ãƒ¼:"):
            katakana_chars = len([c for c in output_text if '\u30A0' <= c <= '\u30FF'])
            # æ—¥æœ¬èªæ–‡å­—ã®ã¿ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå¥èª­ç‚¹ãƒ»è¨˜å·ã‚’é™¤å¤–ï¼‰
            japanese_chars = len([c for c in output_text if (
                '\u3040' <= c <= '\u309F' or  # ã²ã‚‰ãŒãª
                '\u30A0' <= c <= '\u30FF' or  # ã‚«ã‚¿ã‚«ãƒŠ
                '\u4E00' <= c <= '\u9FAF' or  # æ¼¢å­—
                c.isalnum()                   # è‹±æ•°å­—
            )])
            katakana_ratio = katakana_chars / max(japanese_chars, 1)
            
            # æ–‡å­—ç¨®åˆ¥è©³ç´°åˆ†æ
            hiragana_chars = len([c for c in output_text if '\u3040' <= c <= '\u309F'])
            kanji_chars = len([c for c in output_text if '\u4E00' <= c <= '\u9FAF'])
            
            print(f"   ã‚«ã‚¿ã‚«ãƒŠç‡: {katakana_ratio:.1%} ({katakana_chars}/{japanese_chars}æ–‡å­—)")
            if hiragana_chars > 0:
                print(f"   âš ï¸ ã²ã‚‰ãŒãª: {hiragana_chars}æ–‡å­—")
            if kanji_chars > 0:
                print(f"   âš ï¸ æ¼¢å­—: {kanji_chars}æ–‡å­—")
            if inference_time > 0:
                print(f"   å‡¦ç†é€Ÿåº¦: {len(input_text)/inference_time:.1f} æ–‡å­—/ç§’")
        
        print("-" * 40)

def file_mode(model, tokenizer, input_file: str, output_file: str):
    """ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ¢ãƒ¼ãƒ‰"""
    print(f"ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {input_file} â†’ {output_file}")
    
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f.readlines() if line.strip()]
        
        print(f"ğŸ“– {len(lines)}è¡Œã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        
        results = []
        total_time = 0
        for i, line in enumerate(lines, 1):
            try:
                result, inference_time = kanji_to_katakana(model, tokenizer, line)
                total_time += inference_time
                results.append((result, inference_time))
                print(f"âœ… {i}/{len(lines)} å®Œäº†: {line[:30]}... ({inference_time:.3f}ç§’)")
            except Exception as e:
                results.append((f"ã‚¨ãƒ©ãƒ¼: {e}", 0))
                print(f"âŒ {i}/{len(lines)} ã‚¨ãƒ©ãƒ¼: {line[:30]}...")
        
        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"ğŸ“Š ç·å‡¦ç†æ™‚é–“: {total_time:.3f}ç§’\n")
            f.write(f"âš¡ å¹³å‡å‡¦ç†æ™‚é–“: {total_time/len(lines):.3f}ç§’/æ–‡\n")
            f.write(f"ğŸš€ å¹³å‡å‡¦ç†é€Ÿåº¦: {sum(len(line) for line in lines)/total_time:.1f} æ–‡å­—/ç§’\n")
            f.write("=" * 60 + "\n\n")
            
            for original, (converted, inference_time) in zip(lines, results):
                f.write(f"å…¥åŠ›: {original}\n")
                f.write(f"å‡ºåŠ›: {converted}\n")
                if inference_time > 0:
                    f.write(f"æ¨è«–æ™‚é–“: {inference_time:.3f}ç§’\n")
                    f.write(f"å‡¦ç†é€Ÿåº¦: {len(original)/inference_time:.1f} æ–‡å­—/ç§’\n")
                f.write("-" * 40 + "\n")
        
        print(f"âœ… çµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        print(f"ğŸ“Š ç·å‡¦ç†æ™‚é–“: {total_time:.3f}ç§’")
        print(f"ğŸš€ å¹³å‡å‡¦ç†é€Ÿåº¦: {sum(len(line) for line in lines)/total_time:.1f} æ–‡å­—/ç§’")
        
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ Gemma-3-1B æ¼¢å­—â†’ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›æ¨è«–ã‚·ã‚¹ãƒ†ãƒ ")
    print("="*60)
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãƒã‚§ãƒƒã‚¯
    if len(sys.argv) > 1:
        if sys.argv[1] == '--help' or sys.argv[1] == '-h':
            print("ä½¿ã„æ–¹:")
            print("  python inference_gemma3_kanji2kana.py                    # å¯¾è©±ãƒ¢ãƒ¼ãƒ‰")
            print("  python inference_gemma3_kanji2kana.py input.txt output.txt  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰")
            print("  python inference_gemma3_kanji2kana.py --model path/to/model  # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«")
            return
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹è¨­å®šï¼ˆmergedãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§é«˜é€Ÿæ¨è«–ï¼‰
    model_path = "model/gemma3_1b_kanji2kana_sft_merged"
    
    # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹æŒ‡å®š
    if len(sys.argv) >= 3 and sys.argv[1] == '--model':
        model_path = sys.argv[2]
        print(f"ğŸ“‚ ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {model_path}")
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model, tokenizer = load_trained_model(model_path)
    if model is None:
        return
    
    # GPUä½¿ç”¨é‡ç¢ºèª
    if torch.cuda.is_available():
        print(f"ğŸ–¥ï¸ GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ¤å®š
    if len(sys.argv) == 3 and sys.argv[1] != '--model':
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰
        input_file = sys.argv[1]
        output_file = sys.argv[2]
        file_mode(model, tokenizer, input_file, output_file)
    else:
        # å¯¾è©±ãƒ¢ãƒ¼ãƒ‰
        interactive_mode(model, tokenizer)

if __name__ == "__main__":
    main() 